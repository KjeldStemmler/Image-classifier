{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\7Zylo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from fxpmath import Fxp\n",
    "import copy\n",
    "import math\n",
    "import itertools as iter\n",
    "\n",
    "(trainingImages, trainingLabels), (testImages, testLabels) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method for quantizing a numpy array\n",
    "def quantisation(inputArray, bitSize, fractBits = 0):\n",
    "     #compute scale and zeroPoint\n",
    "    maximum = np.max(inputArray);\n",
    "    minimum = np.min(inputArray);\n",
    "    valueRange = maximum - minimum;\n",
    "    \n",
    "    #using a fxpMath dummy to get the bounds for quantisation\n",
    "    fxDummy = Fxp(1,signed = (minimum < 0), n_word = bitSize, n_frac = fractBits)\n",
    "    maxFpVal = fxDummy.upper\n",
    "    minFpVal = fxDummy.lower\n",
    "\n",
    "    #scale = 1 when only one value is present to avoid division by 0\n",
    "    scale = 1;\n",
    "    if (valueRange != 0):\n",
    "        scale = (maxFpVal - minFpVal) / valueRange;\n",
    "    zeroPoint = minFpVal-(scale*minimum);\n",
    "    \n",
    "    #the actual quantisation and restructuring into a fxp-array\n",
    "    output = Fxp((inputArray * scale) + zeroPoint, (minimum < 0), bitSize, fractBits)\n",
    "\n",
    "    return output;\n",
    "\n",
    "def fixedPointConversion(inputArray, bitSize, fractBits = 0):\n",
    "    output = Fxp(inputArray, signed= True, n_word = bitSize, n_frac = fractBits)\n",
    "    return (output)\n",
    "\n",
    "def computeFractBits(weights, biases):\n",
    "    #get only absolute fractional values of the layer\n",
    "    weights = np.abs(np.modf(weights)[0])\n",
    "    weights = np.where(weights == 0, 1, weights)\n",
    "    biases = np.abs(np.modf(biases)[0])\n",
    "    biases = np.where(biases == 0, 1, biases)\n",
    "\n",
    "    #list for storing the sum of the values\n",
    "    sums = [np.sum(weights) + np.sum(biases)]\n",
    "\n",
    "    #range can be adjusted\n",
    "    #continously remove 0.5 the doubles all values\n",
    "    #(if only ideal values are wanted, log2 can also be used)\n",
    "    for i in range (16):\n",
    "        weights = np.where(weights > 0.5, weights - 0.5, weights)\n",
    "        biases = np.where(biases > 0.5, biases - 0.5, biases)\n",
    "        sums.append(np.sum(weights) + np.sum(biases))\n",
    "        weights = weights * 2\n",
    "        biases = biases * 2\n",
    "\n",
    "    #returns the index, where the Values stabilize \n",
    "    stableValue = sums[len(sums) - 1] \n",
    "    for i in range(len(sums)):\n",
    "        if (sums[i] == stableValue):\n",
    "            return i\n",
    "    \n",
    "    #return value, to catch non-stabilizing Weights\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Composited function\n",
    "def quantizeModel(model, testImages, testLabels, quantizationMethod = \"linear\", evaluationMethod = \"bestAccuracy\", tolerance = 0):\n",
    "    \n",
    "    #set up needed variables\n",
    "    #copy of model for testing current weights\n",
    "    modelCpy = copy.deepcopy(model)\n",
    "    layerCount = len(model.layers)\n",
    "    fractBitsSearchSpace = [2,4,8]\n",
    "    #list for the computed needed decimal bits\n",
    "    decimalBits = []\n",
    "    #list of indices of layers with weights\n",
    "    nonEmptyLayerIdx = []\n",
    "    #storage of the chosen quantizatin bits\n",
    "    chosenParameters = []\n",
    "\n",
    "    #get indices of all layers with weights\n",
    "    for idx in range(layerCount):\n",
    "        if (modelCpy.layers[idx].get_weights() != []):\n",
    "            nonEmptyLayerIdx.append(idx)\n",
    "\n",
    "    print(\"non-empty-Layers:\")\n",
    "    print(nonEmptyLayerIdx)\n",
    "\n",
    "    #determine the needed decimal bits\n",
    "    for layerIdx in range(layerCount):\n",
    "        #case for empty layers\n",
    "        if not(layerIdx in nonEmptyLayerIdx):\n",
    "            decimalBits.append(0)\n",
    "        else:\n",
    "            #determine log2 of maximum range\n",
    "            minimum = math.floor(min(np.min(modelCpy.layers[layerIdx].get_weights()[0]), np.min(modelCpy.layers[layerIdx].get_weights()[1])))  \n",
    "            maximum = math.ceil(max(np.max(modelCpy.layers[layerIdx].get_weights()[0]), np.max(modelCpy.layers[layerIdx].get_weights()[1])))\n",
    "            valueRange = max(maximum, 0) - min(minimum, 0)\n",
    "            #safety check\n",
    "            if (valueRange > 0):\n",
    "                decimalBits.append(math.ceil(math.log(valueRange, 2)))\n",
    "            else:\n",
    "                print(\"Error: Value Range is 0 or negative\")\n",
    "\n",
    "    print(\"decimal Bits:\")\n",
    "    print(decimalBits)\n",
    "    \n",
    "    ##run chosen quantization method\n",
    "    match quantizationMethod:\n",
    "        case \"linear\":\n",
    "            chosenBits =[]\n",
    "            for layerIdx in nonEmptyLayerIdx:\n",
    "                currentFractBits = computeFractBits(copy.deepcopy(model.layers[layerIdx].get_weights()[0]), copy.deepcopy(model.layers[layerIdx].get_weights()[1]))\n",
    "                chosenBits.append(currentFractBits + decimalBits[layerIdx])\n",
    "                newLayer = []\n",
    "                newLayer.append(fixedPointConversion(copy.deepcopy(model.layers[layerIdx].get_weights()[0]), decimalBits[layerIdx] + currentFractBits, currentFractBits))\n",
    "                newLayer.append(fixedPointConversion(copy.deepcopy(model.layers[layerIdx].get_weights()[1]), decimalBits[layerIdx] + currentFractBits, currentFractBits))\n",
    "                modelCpy.layers[layerIdx].set_weights(newLayer)\n",
    "            test_loss, test_acc = modelCpy.evaluate(testImages,  testLabels, verbose=0)\n",
    "            chosenParameters = chosenBits\n",
    "        case \"quadratic\":\n",
    "            #optimizes layer by layer\n",
    "            for currentLayer in nonEmptyLayerIdx:\n",
    "                accuracies = []\n",
    "                originalLayer = copy.deepcopy(model.layers[currentLayer].get_weights())\n",
    "                #iterates over specified numberfs of fractional bits, and specified number of overall bits for each\n",
    "                for fractBitsIdx in range(len(fractBitsSearchSpace)):\n",
    "                            #initializes a new container for all weights of the current layer\n",
    "                            newLayer = []\n",
    "                            #iterates over all sets of weights in layer (needed, because the list of all weights has inhomogenus shape, so numpy cant handle it)\n",
    "                            newLayer.append(fixedPointConversion(copy.deepcopy(originalLayer[0]), decimalBits[layerIdx], fractBitsSearchSpace[fractBitsIdx]))\n",
    "                            newLayer.append(fixedPointConversion(copy.deepcopy(originalLayer[1]), decimalBits[layerIdx], fractBitsSearchSpace[fractBitsIdx]))\n",
    "                            #sets the new layer\n",
    "                            modelCpy.layers[currentLayer].set_weights(newLayer)\n",
    "                            #tests the new accuracy and stores it in a matrix\n",
    "                            test_loss, test_acc = modelCpy.evaluate(testImages,  testLabels, verbose=0)\n",
    "                            accuracies.append(test_acc)\n",
    "\n",
    "                #evaluate current layer based on chosen method\n",
    "                bestAccuracyIndex = 0\n",
    "                match evaluationMethod:                    \n",
    "                    case \"ratio\":\n",
    "                    #determine best suited Values by computing the ratios of bits to accuracy\n",
    "                    #exclude accuracies below a certain border (tolerance)\n",
    "                        ratios = []\n",
    "                        minimumAccuracy = max(accuracies) - tolerance\n",
    "                        for idx in range(len(accuracies)):\n",
    "                            if (accuracies[idx] < minimumAccuracy):\n",
    "                                ratios.append(0)\n",
    "                            else:\n",
    "                                ratios.append(fractBitsSearchSpace[idx] / accuracies[idx])\n",
    "                        bestAccuracyIndex = ratios.index(max(ratios))\n",
    "                    case \"toleranceBorder\":\n",
    "                        #only update, if new accuracies exceeds old one by a chosen amount (tolerance)\n",
    "                        for idx in range(len(accuracies)):\n",
    "                            if ((accuracies[idx] - tolerance) > accuracies[bestAccuracyIndex]):\n",
    "                                bestAccuracyIndex = idx\n",
    "                    case \"bestAccuracy\":\n",
    "                        #simply determine best accuracy\n",
    "                        bestAccuracyIndex = accuracies.index(max(accuracies))\n",
    "                    case _:\n",
    "                        print(\"Error in matching choosen evaluation method\")\n",
    "                        print(\"should be either ratio, toleranceBorder, or bestAcccuracy, but is: \" + evaluationMethod)\n",
    "                        return -1\n",
    "                    \n",
    "                chosenParameters.append(fractBitsSearchSpace[bestAccuracyIndex])\n",
    "\n",
    "        case \"exponential\":\n",
    "            #iterate over all permutations and test the accuracy\n",
    "            permutations = list(iter.product(fractBitsSearchSpace, repeat = len(nonEmptyLayerIdx)))\n",
    "            print(\"goal: \" + str(len(permutations)) + \" permutations\")\n",
    "            accuracies = []\n",
    "            for permutation in range(len(permutations)):\n",
    "                for layerIdx in nonEmptyLayerIdx:\n",
    "                    currentFractBits = permutations[permutation][nonEmptyLayerIdx.index(layerIdx)]\n",
    "                    newLayer = []\n",
    "                    newLayer.append(fixedPointConversion(copy.deepcopy(model.layers[layerIdx].get_weights()[0]), decimalBits[layerIdx] + currentFractBits, currentFractBits))\n",
    "                    newLayer.append(fixedPointConversion(copy.deepcopy(model.layers[layerIdx].get_weights()[1]), decimalBits[layerIdx] + currentFractBits, currentFractBits))\n",
    "                    modelCpy.layers[layerIdx].set_weights(newLayer)\n",
    "                test_loss, test_acc = modelCpy.evaluate(testImages,  testLabels, verbose=0)\n",
    "                accuracies.append(test_acc)\n",
    "                print(\"run: \" + str(permutation))\n",
    "                print(test_acc)\n",
    "\n",
    "            print (\"length of accuracies and permutations. should be equal:\")\n",
    "            print(len(accuracies), len(permutations))\n",
    "\n",
    "            ##determine best parameter by maximizing accuracy by chosen method\n",
    "            bestAccuracyIndex = 0\n",
    "            match evaluationMethod:\n",
    "                case \"ratio\":\n",
    "                    #determine best suited Values by computing the ratios of bits to accuracy\n",
    "                    #exclude accuracies below a certain border (tolerance)\n",
    "                    ratios = []\n",
    "                    minimumAccuracy = max(accuracies) - tolerance\n",
    "                    for idx in range(len(accuracies)):\n",
    "                        if (accuracies[idx] < minimumAccuracy):\n",
    "                            ratios.append(0)\n",
    "                        else:\n",
    "                            ratios.append(sum(permutations[idx]) / accuracies[idx])\n",
    "                    bestAccuracyIndex = ratios.index(max(ratios))\n",
    "                case \"toleranceBorder\":\n",
    "                    #only update, if new accuracy exceeds old one by a chosen amount (tolerance)\n",
    "                    for idx in range(len(accuracies)):\n",
    "                        if ((accuracies[idx] - tolerance) > accuracies[bestAccuracyIndex]):\n",
    "                            bestAccuracyIndex = idx\n",
    "                case \"bestAccuracy\":\n",
    "                    #simply determine best accuracy\n",
    "                    bestAccuracyIndex = accuracies.index(max(accuracies))\n",
    "                case _:\n",
    "                    print(\"Error in matching choosen evaluation method\")\n",
    "                    print(\"should be either ratio, toleranceBorder, or bestAcccuracy, but is: \" + evaluationMethod)\n",
    "                    return -1\n",
    "                \n",
    "            chosenParameters = permutations[bestAccuracyIndex]\n",
    "        case _:\n",
    "            print(\"Error in matching choosen quantisation method\")\n",
    "            print(\"should be either linear, quadratic, or exponential, but is: \" + quantizationMethod)\n",
    "            return -1            \n",
    "\n",
    "    \n",
    "        \n",
    "    #apply quantisation to model and return\n",
    "    print(chosenParameters)\n",
    "    for layerIdx in nonEmptyLayerIdx:\n",
    "        currentFractBits = chosenParameters[nonEmptyLayerIdx.index(layerIdx)]\n",
    "        newLayer = []\n",
    "        newLayer.append(fixedPointConversion(copy.deepcopy(model.layers[layerIdx].get_weights()[0]), decimalBits[layerIdx] + currentFractBits, currentFractBits))\n",
    "        newLayer.append(fixedPointConversion(copy.deepcopy(model.layers[layerIdx].get_weights()[1]), decimalBits[layerIdx] + currentFractBits, currentFractBits))\n",
    "        modelCpy.layers[layerIdx].set_weights(newLayer)\n",
    "\n",
    "    test_loss, test_acc = modelCpy.evaluate(testImages,  testLabels, verbose=0)\n",
    "    print(\"new accuracy\")\n",
    "    print(test_acc)\n",
    "    print(\"mean used Bits\")\n",
    "    print(sum(chosenParameters)/len(chosenParameters))\n",
    "\n",
    "    return modelCpy\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\7Zylo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\7Zylo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 15, 15, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 15, 15, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 6, 6, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 6, 6, 64)          0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                65600     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 124330 (485.66 KB)\n",
      "Trainable params: 124330 (485.66 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From c:\\Users\\7Zylo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From c:\\Users\\7Zylo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\7Zylo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 1.5763 - accuracy: 0.4182 - val_loss: 1.3365 - val_accuracy: 0.5037\n",
      "Epoch 2/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.2148 - accuracy: 0.5636 - val_loss: 1.1167 - val_accuracy: 0.6039\n",
      "Epoch 3/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 1.0716 - accuracy: 0.6219 - val_loss: 0.9899 - val_accuracy: 0.6541\n",
      "Epoch 4/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.9787 - accuracy: 0.6561 - val_loss: 0.9386 - val_accuracy: 0.6719\n",
      "Epoch 5/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.9061 - accuracy: 0.6826 - val_loss: 0.9365 - val_accuracy: 0.6734\n",
      "Epoch 6/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.8517 - accuracy: 0.7000 - val_loss: 0.8517 - val_accuracy: 0.7040\n",
      "Epoch 7/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.8168 - accuracy: 0.7138 - val_loss: 0.8578 - val_accuracy: 0.7025\n",
      "Epoch 8/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7838 - accuracy: 0.7244 - val_loss: 0.8381 - val_accuracy: 0.7111\n",
      "Epoch 9/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7546 - accuracy: 0.7346 - val_loss: 0.8475 - val_accuracy: 0.7095\n",
      "Epoch 10/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7252 - accuracy: 0.7442 - val_loss: 0.8041 - val_accuracy: 0.7225\n",
      "Epoch 11/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7070 - accuracy: 0.7508 - val_loss: 0.7832 - val_accuracy: 0.7297\n",
      "Epoch 12/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.6850 - accuracy: 0.7576 - val_loss: 0.7998 - val_accuracy: 0.7297\n",
      "Epoch 13/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.6613 - accuracy: 0.7676 - val_loss: 0.7957 - val_accuracy: 0.7298\n",
      "Epoch 14/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.6502 - accuracy: 0.7693 - val_loss: 0.7932 - val_accuracy: 0.7339\n",
      "Epoch 15/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.6339 - accuracy: 0.7772 - val_loss: 0.7902 - val_accuracy: 0.7376\n",
      "Epoch 16/20\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.6183 - accuracy: 0.7813 - val_loss: 0.7906 - val_accuracy: 0.7312\n",
      "Epoch 17/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.6068 - accuracy: 0.7839 - val_loss: 0.8214 - val_accuracy: 0.7270\n",
      "Epoch 18/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.5944 - accuracy: 0.7879 - val_loss: 0.8201 - val_accuracy: 0.7293\n",
      "Epoch 19/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.5830 - accuracy: 0.7933 - val_loss: 0.8240 - val_accuracy: 0.7267\n",
      "Epoch 20/20\n",
      "1563/1563 [==============================] - 10s 7ms/step - loss: 0.5684 - accuracy: 0.7985 - val_loss: 0.8053 - val_accuracy: 0.7347\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.8053 - accuracy: 0.7347 - 550ms/epoch - 2ms/step\n",
      "0.7347000241279602\n"
     ]
    }
   ],
   "source": [
    "#normalizing the values to 0-1\n",
    "trainingImages = trainingImages / 255.0\n",
    "testImages = testImages / 255.0\n",
    "\n",
    "# One hot encoding the target class (labels)\n",
    "num_classes = 10\n",
    "trainingLabels = tf.keras.utils.to_categorical(trainingLabels, num_classes)\n",
    "testLabels = tf.keras.utils.to_categorical(testLabels, num_classes)\n",
    "\n",
    "#convolution layers for feature extraction\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "\n",
    "\n",
    "#fully connected layers for class prediction\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "#print the current model summary\n",
    "model.summary()\n",
    "\n",
    "#preparing model for training\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#training the model\n",
    "history = model.fit(trainingImages, trainingLabels, epochs=20, \n",
    "                    validation_data=(testImages, testLabels))\n",
    "\n",
    "#set to True to display the changing accuracy of the model during training\n",
    "if(True):\n",
    "    plt.plot(history.history['accuracy'], label='accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "#display the accuracy of the trained model\n",
    "test_loss, test_acc = model.evaluate(testImages,  testLabels, verbose=2)\n",
    "print(test_acc)\n",
    "\n",
    "#get the weights of the model \n",
    "weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_classifier\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_classifier\\assets\n"
     ]
    }
   ],
   "source": [
    "#save model\n",
    "model.save(\"trained_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\7Zylo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:107: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\7Zylo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\7Zylo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:585: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "model = tf.keras.models.load_model(\"trained_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 25 variables whereas the saved optimizer has 1 variables. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-empty-Layers:\n",
      "[0, 3, 6, 9, 10, 11]\n",
      "decimal Bits:\n",
      "[1, 0, 0, 2, 0, 0, 2, 0, 0, 2, 1, 2]\n",
      "goal: 729 permutations\n",
      "run: 0\n",
      "0.10000000149011612\n",
      "run: 1\n",
      "0.10000000149011612\n",
      "run: 2\n",
      "0.10000000149011612\n",
      "run: 3\n",
      "0.10000000149011612\n",
      "run: 4\n",
      "0.10000000149011612\n",
      "run: 5\n",
      "0.10000000149011612\n",
      "run: 6\n",
      "0.10000000149011612\n",
      "run: 7\n",
      "0.09960000216960907\n",
      "run: 8\n",
      "0.0997999981045723\n",
      "run: 9\n",
      "0.10000000149011612\n",
      "run: 10\n",
      "0.10000000149011612\n",
      "run: 11\n",
      "0.10000000149011612\n",
      "run: 12\n",
      "0.10000000149011612\n",
      "run: 13\n",
      "0.10000000149011612\n",
      "run: 14\n",
      "0.10000000149011612\n",
      "run: 15\n",
      "0.10000000149011612\n",
      "run: 16\n",
      "0.09989999979734421\n",
      "run: 17\n",
      "0.09989999979734421\n",
      "run: 18\n",
      "0.10000000149011612\n",
      "run: 19\n",
      "0.10000000149011612\n",
      "run: 20\n",
      "0.10000000149011612\n",
      "run: 21\n",
      "0.10000000149011612\n",
      "run: 22\n",
      "0.10000000149011612\n",
      "run: 23\n",
      "0.10000000149011612\n",
      "run: 24\n",
      "0.10010000318288803\n",
      "run: 25\n",
      "0.09989999979734421\n",
      "run: 26\n",
      "0.10000000149011612\n",
      "run: 27\n",
      "0.10000000149011612\n",
      "run: 28\n",
      "0.10000000149011612\n",
      "run: 29\n",
      "0.10000000149011612\n",
      "run: 30\n",
      "0.10000000149011612\n",
      "run: 31\n",
      "0.10000000149011612\n",
      "run: 32\n",
      "0.10000000149011612\n",
      "run: 33\n",
      "0.10000000149011612\n",
      "run: 34\n",
      "0.10019999742507935\n",
      "run: 35\n",
      "0.10010000318288803\n",
      "run: 36\n",
      "0.10000000149011612\n",
      "run: 37\n",
      "0.10000000149011612\n",
      "run: 38\n",
      "0.09989999979734421\n",
      "run: 39\n",
      "0.09989999979734421\n",
      "run: 40\n",
      "0.10300000011920929\n",
      "run: 41\n",
      "0.10379999876022339\n",
      "run: 42\n",
      "0.10010000318288803\n",
      "run: 43\n",
      "0.10180000215768814\n",
      "run: 44\n",
      "0.0997999981045723\n",
      "run: 45\n",
      "0.10000000149011612\n",
      "run: 46\n",
      "0.10000000149011612\n",
      "run: 47\n",
      "0.10000000149011612\n",
      "run: 48\n",
      "0.10000000149011612\n",
      "run: 49\n",
      "0.10369999706745148\n",
      "run: 50\n",
      "0.10369999706745148\n",
      "run: 51\n",
      "0.10010000318288803\n",
      "run: 52\n",
      "0.10170000046491623\n",
      "run: 53\n",
      "0.10090000182390213\n",
      "run: 54\n",
      "0.10000000149011612\n",
      "run: 55\n",
      "0.10000000149011612\n",
      "run: 56\n",
      "0.10000000149011612\n",
      "run: 57\n",
      "0.10000000149011612\n",
      "run: 58\n",
      "0.10000000149011612\n",
      "run: 59\n",
      "0.10000000149011612\n",
      "run: 60\n",
      "0.10000000149011612\n",
      "run: 61\n",
      "0.09989999979734421\n",
      "run: 62\n",
      "0.1006999984383583\n",
      "run: 63\n",
      "0.10000000149011612\n",
      "run: 64\n",
      "0.10000000149011612\n",
      "run: 65\n",
      "0.09989999979734421\n",
      "run: 66\n",
      "0.0997999981045723\n",
      "run: 67\n",
      "0.10350000113248825\n",
      "run: 68\n",
      "0.10589999705553055\n",
      "run: 69\n",
      "0.0997999981045723\n",
      "run: 70\n",
      "0.10400000214576721\n",
      "run: 71\n",
      "0.10909999907016754\n",
      "run: 72\n",
      "0.10000000149011612\n",
      "run: 73\n",
      "0.10000000149011612\n",
      "run: 74\n",
      "0.10000000149011612\n",
      "run: 75\n",
      "0.10000000149011612\n",
      "run: 76\n",
      "0.10379999876022339\n",
      "run: 77\n",
      "0.11879999935626984\n",
      "run: 78\n",
      "0.10000000149011612\n",
      "run: 79\n",
      "0.10000000149011612\n",
      "run: 80\n",
      "0.11900000274181366\n",
      "run: 81\n",
      "0.10000000149011612\n",
      "run: 82\n",
      "0.10000000149011612\n",
      "run: 83\n",
      "0.10000000149011612\n",
      "run: 84\n",
      "0.10000000149011612\n",
      "run: 85\n",
      "0.10300000011920929\n",
      "run: 86\n",
      "0.10620000213384628\n",
      "run: 87\n",
      "0.10000000149011612\n",
      "run: 88\n",
      "0.10239999741315842\n",
      "run: 89\n",
      "0.10209999978542328\n",
      "run: 90\n",
      "0.10000000149011612\n",
      "run: 91\n",
      "0.10000000149011612\n",
      "run: 92\n",
      "0.10000000149011612\n",
      "run: 93\n",
      "0.09769999980926514\n",
      "run: 94\n",
      "0.10019999742507935\n",
      "run: 95\n",
      "0.10010000318288803\n",
      "run: 96\n",
      "0.0860000029206276\n",
      "run: 97\n",
      "0.10000000149011612\n",
      "run: 98\n",
      "0.10000000149011612\n",
      "run: 99\n",
      "0.10000000149011612\n",
      "run: 100\n",
      "0.10000000149011612\n",
      "run: 101\n",
      "0.10000000149011612\n",
      "run: 102\n",
      "0.09929999709129333\n",
      "run: 103\n",
      "0.10010000318288803\n",
      "run: 104\n",
      "0.10000000149011612\n",
      "run: 105\n",
      "0.09700000286102295\n",
      "run: 106\n",
      "0.10000000149011612\n",
      "run: 107\n",
      "0.10000000149011612\n",
      "run: 108\n",
      "0.10000000149011612\n",
      "run: 109\n",
      "0.10000000149011612\n",
      "run: 110\n",
      "0.10000000149011612\n",
      "run: 111\n",
      "0.10010000318288803\n",
      "run: 112\n",
      "0.09989999979734421\n",
      "run: 113\n",
      "0.09759999811649323\n",
      "run: 114\n",
      "0.10000000149011612\n",
      "run: 115\n",
      "0.09749999642372131\n",
      "run: 116\n",
      "0.09769999980926514\n",
      "run: 117\n",
      "0.10000000149011612\n",
      "run: 118\n",
      "0.10000000149011612\n",
      "run: 119\n",
      "0.10000000149011612\n",
      "run: 120\n",
      "0.10000000149011612\n",
      "run: 121\n",
      "0.09920000284910202\n",
      "run: 122\n",
      "0.09849999845027924\n",
      "run: 123\n",
      "0.10000000149011612\n",
      "run: 124\n",
      "0.09910000115633011\n",
      "run: 125\n",
      "0.09950000047683716\n",
      "run: 126\n",
      "0.10000000149011612\n",
      "run: 127\n",
      "0.10000000149011612\n",
      "run: 128\n",
      "0.10000000149011612\n",
      "run: 129\n",
      "0.0997999981045723\n",
      "run: 130\n",
      "0.10509999841451645\n",
      "run: 131\n",
      "0.09880000352859497\n",
      "run: 132\n",
      "0.09049999713897705\n",
      "run: 133\n",
      "0.09950000047683716\n",
      "run: 134\n",
      "0.09960000216960907\n",
      "run: 135\n",
      "0.10000000149011612\n",
      "run: 136\n",
      "0.10000000149011612\n",
      "run: 137\n",
      "0.10000000149011612\n",
      "run: 138\n",
      "0.09989999979734421\n",
      "run: 139\n",
      "0.09830000251531601\n",
      "run: 140\n",
      "0.09740000218153\n",
      "run: 141\n",
      "0.10000000149011612\n",
      "run: 142\n",
      "0.0989999994635582\n",
      "run: 143\n",
      "0.09880000352859497\n",
      "run: 144\n",
      "0.10000000149011612\n",
      "run: 145\n",
      "0.10000000149011612\n",
      "run: 146\n",
      "0.10000000149011612\n",
      "run: 147\n",
      "0.10000000149011612\n",
      "run: 148\n",
      "0.09989999979734421\n",
      "run: 149\n",
      "0.10010000318288803\n",
      "run: 150\n",
      "0.0989999994635582\n",
      "run: 151\n",
      "0.09920000284910202\n",
      "run: 152\n",
      "0.09830000251531601\n",
      "run: 153\n",
      "0.10000000149011612\n",
      "run: 154\n",
      "0.10000000149011612\n",
      "run: 155\n",
      "0.10000000149011612\n",
      "run: 156\n",
      "0.09780000150203705\n",
      "run: 157\n",
      "0.10580000281333923\n",
      "run: 158\n",
      "0.10019999742507935\n",
      "run: 159\n",
      "0.08489999920129776\n",
      "run: 160\n",
      "0.09939999878406525\n",
      "run: 161\n",
      "0.09809999912977219\n",
      "run: 162\n",
      "0.10000000149011612\n",
      "run: 163\n",
      "0.10000000149011612\n",
      "run: 164\n",
      "0.10000000149011612\n",
      "run: 165\n",
      "0.10000000149011612\n",
      "run: 166\n",
      "0.10249999910593033\n",
      "run: 167\n",
      "0.10270000249147415\n",
      "run: 168\n",
      "0.09790000319480896\n",
      "run: 169\n",
      "0.1006999984383583\n",
      "run: 170\n",
      "0.10080000013113022\n",
      "run: 171\n",
      "0.10000000149011612\n",
      "run: 172\n",
      "0.10000000149011612\n",
      "run: 173\n",
      "0.10000000149011612\n",
      "run: 174\n",
      "0.09950000047683716\n",
      "run: 175\n",
      "0.09989999979734421\n",
      "run: 176\n",
      "0.0997999981045723\n",
      "run: 177\n",
      "0.09780000150203705\n",
      "run: 178\n",
      "0.10000000149011612\n",
      "run: 179\n",
      "0.10000000149011612\n",
      "run: 180\n",
      "0.10000000149011612\n",
      "run: 181\n",
      "0.10000000149011612\n",
      "run: 182\n",
      "0.10000000149011612\n",
      "run: 183\n",
      "0.09969999641180038\n",
      "run: 184\n",
      "0.10000000149011612\n",
      "run: 185\n",
      "0.09989999979734421\n",
      "run: 186\n",
      "0.1023000031709671\n",
      "run: 187\n",
      "0.10000000149011612\n",
      "run: 188\n",
      "0.10000000149011612\n",
      "run: 189\n",
      "0.10000000149011612\n",
      "run: 190\n",
      "0.10000000149011612\n",
      "run: 191\n",
      "0.10000000149011612\n",
      "run: 192\n",
      "0.10000000149011612\n",
      "run: 193\n",
      "0.09830000251531601\n",
      "run: 194\n",
      "0.09790000319480896\n",
      "run: 195\n",
      "0.09989999979734421\n",
      "run: 196\n",
      "0.09790000319480896\n",
      "run: 197\n",
      "0.09910000115633011\n",
      "run: 198\n",
      "0.10000000149011612\n",
      "run: 199\n",
      "0.10000000149011612\n",
      "run: 200\n",
      "0.10000000149011612\n",
      "run: 201\n",
      "0.10000000149011612\n",
      "run: 202\n",
      "0.10019999742507935\n",
      "run: 203\n",
      "0.09950000047683716\n",
      "run: 204\n",
      "0.09839999675750732\n",
      "run: 205\n",
      "0.09960000216960907\n",
      "run: 206\n",
      "0.0982000008225441\n",
      "run: 207\n",
      "0.10000000149011612\n",
      "run: 208\n",
      "0.10000000149011612\n",
      "run: 209\n",
      "0.10000000149011612\n",
      "run: 210\n",
      "0.09730000048875809\n",
      "run: 211\n",
      "0.10899999737739563\n",
      "run: 212\n",
      "0.10019999742507935\n",
      "run: 213\n",
      "0.09030000120401382\n",
      "run: 214\n",
      "0.10010000318288803\n",
      "run: 215\n",
      "0.09809999912977219\n",
      "run: 216\n",
      "0.10000000149011612\n",
      "run: 217\n",
      "0.10000000149011612\n",
      "run: 218\n",
      "0.10000000149011612\n",
      "run: 219\n",
      "0.0997999981045723\n",
      "run: 220\n",
      "0.09830000251531601\n",
      "run: 221\n",
      "0.09830000251531601\n",
      "run: 222\n",
      "0.09989999979734421\n",
      "run: 223\n",
      "0.0989999994635582\n",
      "run: 224\n",
      "0.0997999981045723\n",
      "run: 225\n",
      "0.10000000149011612\n",
      "run: 226\n",
      "0.10000000149011612\n",
      "run: 227\n",
      "0.10000000149011612\n",
      "run: 228\n",
      "0.0997999981045723\n",
      "run: 229\n",
      "0.1005999967455864\n",
      "run: 230\n",
      "0.10209999978542328\n",
      "run: 231\n",
      "0.09679999947547913\n",
      "run: 232\n",
      "0.0997999981045723\n",
      "run: 233\n",
      "0.09239999949932098\n",
      "run: 234\n",
      "0.10000000149011612\n",
      "run: 235\n",
      "0.10000000149011612\n",
      "run: 236\n",
      "0.10000000149011612\n",
      "run: 237\n",
      "0.09480000287294388\n",
      "run: 238\n",
      "0.11219999939203262\n",
      "run: 239\n",
      "0.10480000078678131\n",
      "run: 240\n",
      "0.09300000220537186\n",
      "run: 241\n",
      "0.0997999981045723\n",
      "run: 242\n",
      "0.09459999948740005\n",
      "run: 243\n",
      "0.10000000149011612\n",
      "run: 244\n",
      "0.10000000149011612\n",
      "run: 245\n",
      "0.10010000318288803\n",
      "run: 246\n",
      "0.10199999809265137\n",
      "run: 247\n",
      "0.1006999984383583\n",
      "run: 248\n",
      "0.1006999984383583\n",
      "run: 249\n",
      "0.10199999809265137\n",
      "run: 250\n",
      "0.1014999970793724\n",
      "run: 251\n",
      "0.10220000147819519\n",
      "run: 252\n",
      "0.10000000149011612\n",
      "run: 253\n",
      "0.09960000216960907\n",
      "run: 254\n",
      "0.09740000218153\n",
      "run: 255\n",
      "0.10010000318288803\n",
      "run: 256\n",
      "0.09589999914169312\n",
      "run: 257\n",
      "0.09830000251531601\n",
      "run: 258\n",
      "0.09910000115633011\n",
      "run: 259\n",
      "0.09589999914169312\n",
      "run: 260\n",
      "0.1014999970793724\n",
      "run: 261\n",
      "0.10000000149011612\n",
      "run: 262\n",
      "0.09880000352859497\n",
      "run: 263\n",
      "0.09080000221729279\n",
      "run: 264\n",
      "0.1005999967455864\n",
      "run: 265\n",
      "0.09179999679327011\n",
      "run: 266\n",
      "0.09459999948740005\n",
      "run: 267\n",
      "0.10019999742507935\n",
      "run: 268\n",
      "0.0982000008225441\n",
      "run: 269\n",
      "0.09969999641180038\n",
      "run: 270\n",
      "0.10000000149011612\n",
      "run: 271\n",
      "0.10000000149011612\n",
      "run: 272\n",
      "0.10010000318288803\n",
      "run: 273\n",
      "0.10279999673366547\n",
      "run: 274\n",
      "0.10019999742507935\n",
      "run: 275\n",
      "0.10159999877214432\n",
      "run: 276\n",
      "0.10100000351667404\n",
      "run: 277\n",
      "0.10419999808073044\n",
      "run: 278\n",
      "0.1006999984383583\n",
      "run: 279\n",
      "0.10000000149011612\n",
      "run: 280\n",
      "0.10000000149011612\n",
      "run: 281\n",
      "0.09969999641180038\n",
      "run: 282\n",
      "0.0934000015258789\n",
      "run: 283\n",
      "0.1031000018119812\n",
      "run: 284\n",
      "0.10499999672174454\n",
      "run: 285\n",
      "0.09080000221729279\n",
      "run: 286\n",
      "0.10289999842643738\n",
      "run: 287\n",
      "0.09769999980926514\n",
      "run: 288\n",
      "0.10000000149011612\n",
      "run: 289\n",
      "0.10000000149011612\n",
      "run: 290\n",
      "0.09849999845027924\n",
      "run: 291\n",
      "0.08860000222921371\n",
      "run: 292\n",
      "0.10189999639987946\n",
      "run: 293\n",
      "0.10220000147819519\n",
      "run: 294\n",
      "0.0877000018954277\n",
      "run: 295\n",
      "0.10109999775886536\n",
      "run: 296\n",
      "0.09440000355243683\n",
      "run: 297\n",
      "0.10000000149011612\n",
      "run: 298\n",
      "0.10000000149011612\n",
      "run: 299\n",
      "0.10000000149011612\n",
      "run: 300\n",
      "0.10530000180006027\n",
      "run: 301\n",
      "0.10239999741315842\n",
      "run: 302\n",
      "0.10670000314712524\n",
      "run: 303\n",
      "0.10429999977350235\n",
      "run: 304\n",
      "0.10700000077486038\n",
      "run: 305\n",
      "0.10589999705553055\n",
      "run: 306\n",
      "0.10000000149011612\n",
      "run: 307\n",
      "0.10000000149011612\n",
      "run: 308\n",
      "0.10029999911785126\n",
      "run: 309\n",
      "0.08950000256299973\n",
      "run: 310\n",
      "0.09830000251531601\n",
      "run: 311\n",
      "0.09300000220537186\n",
      "run: 312\n",
      "0.08160000294446945\n",
      "run: 313\n",
      "0.09700000286102295\n",
      "run: 314\n",
      "0.08889999985694885\n",
      "run: 315\n",
      "0.10000000149011612\n",
      "run: 316\n",
      "0.10000000149011612\n",
      "run: 317\n",
      "0.09950000047683716\n",
      "run: 318\n",
      "0.08420000225305557\n",
      "run: 319\n",
      "0.0989999994635582\n",
      "run: 320\n",
      "0.09149999916553497\n",
      "run: 321\n",
      "0.08669999986886978\n",
      "run: 322\n",
      "0.0957999974489212\n",
      "run: 323\n",
      "0.0925000011920929\n",
      "run: 324\n",
      "0.10000000149011612\n",
      "run: 325\n",
      "0.10000000149011612\n",
      "run: 326\n",
      "0.10000000149011612\n",
      "run: 327\n",
      "0.10090000182390213\n",
      "run: 328\n",
      "0.10170000046491623\n",
      "run: 329\n",
      "0.1046999990940094\n",
      "run: 330\n",
      "0.10199999809265137\n",
      "run: 331\n",
      "0.10339999943971634\n",
      "run: 332\n",
      "0.11240000277757645\n",
      "run: 333\n",
      "0.10000000149011612\n",
      "run: 334\n",
      "0.09969999641180038\n",
      "run: 335\n",
      "0.1006999984383583\n",
      "run: 336\n",
      "0.10589999705553055\n",
      "run: 337\n",
      "0.11150000244379044\n",
      "run: 338\n",
      "0.11289999634027481\n",
      "run: 339\n",
      "0.10920000076293945\n",
      "run: 340\n",
      "0.12070000171661377\n",
      "run: 341\n",
      "0.11879999935626984\n",
      "run: 342\n",
      "0.10000000149011612\n",
      "run: 343\n",
      "0.09910000115633011\n",
      "run: 344\n",
      "0.10409999638795853\n",
      "run: 345\n",
      "0.11020000278949738\n",
      "run: 346\n",
      "0.11550000309944153\n",
      "run: 347\n",
      "0.1185000017285347\n",
      "run: 348\n",
      "0.1096000000834465\n",
      "run: 349\n",
      "0.12449999898672104\n",
      "run: 350\n",
      "0.12269999831914902\n",
      "run: 351\n",
      "0.10000000149011612\n",
      "run: 352\n",
      "0.10000000149011612\n",
      "run: 353\n",
      "0.10019999742507935\n",
      "run: 354\n",
      "0.10849999636411667\n",
      "run: 355\n",
      "0.12489999830722809\n",
      "run: 356\n",
      "0.1282999962568283\n",
      "run: 357\n",
      "0.10639999806880951\n",
      "run: 358\n",
      "0.14079999923706055\n",
      "run: 359\n",
      "0.14830000698566437\n",
      "run: 360\n",
      "0.10000000149011612\n",
      "run: 361\n",
      "0.10000000149011612\n",
      "run: 362\n",
      "0.10029999911785126\n",
      "run: 363\n",
      "0.1298000067472458\n",
      "run: 364\n",
      "0.18529999256134033\n",
      "run: 365\n",
      "0.1890999972820282\n",
      "run: 366\n",
      "0.1256999969482422\n",
      "run: 367\n",
      "0.24420000612735748\n",
      "run: 368\n",
      "0.21799999475479126\n",
      "run: 369\n",
      "0.10000000149011612\n",
      "run: 370\n",
      "0.09969999641180038\n",
      "run: 371\n",
      "0.10119999945163727\n",
      "run: 372\n",
      "0.1290999948978424\n",
      "run: 373\n",
      "0.22110000252723694\n",
      "run: 374\n",
      "0.20280000567436218\n",
      "run: 375\n",
      "0.11990000307559967\n",
      "run: 376\n",
      "0.241799995303154\n",
      "run: 377\n",
      "0.23280000686645508\n",
      "run: 378\n",
      "0.10000000149011612\n",
      "run: 379\n",
      "0.09989999979734421\n",
      "run: 380\n",
      "0.10050000250339508\n",
      "run: 381\n",
      "0.1143999993801117\n",
      "run: 382\n",
      "0.1339000016450882\n",
      "run: 383\n",
      "0.1348000019788742\n",
      "run: 384\n",
      "0.11299999803304672\n",
      "run: 385\n",
      "0.1445000022649765\n",
      "run: 386\n",
      "0.15410000085830688\n",
      "run: 387\n",
      "0.10000000149011612\n",
      "run: 388\n",
      "0.09989999979734421\n",
      "run: 389\n",
      "0.10180000215768814\n",
      "run: 390\n",
      "0.13449999690055847\n",
      "run: 391\n",
      "0.21699999272823334\n",
      "run: 392\n",
      "0.2003999948501587\n",
      "run: 393\n",
      "0.12690000236034393\n",
      "run: 394\n",
      "0.25839999318122864\n",
      "run: 395\n",
      "0.2345000058412552\n",
      "run: 396\n",
      "0.10000000149011612\n",
      "run: 397\n",
      "0.09969999641180038\n",
      "run: 398\n",
      "0.10379999876022339\n",
      "run: 399\n",
      "0.1315000057220459\n",
      "run: 400\n",
      "0.24269999563694\n",
      "run: 401\n",
      "0.21850000321865082\n",
      "run: 402\n",
      "0.11990000307559967\n",
      "run: 403\n",
      "0.25699999928474426\n",
      "run: 404\n",
      "0.24379999935626984\n",
      "run: 405\n",
      "0.10000000149011612\n",
      "run: 406\n",
      "0.10000000149011612\n",
      "run: 407\n",
      "0.10000000149011612\n",
      "run: 408\n",
      "0.1046999990940094\n",
      "run: 409\n",
      "0.10209999978542328\n",
      "run: 410\n",
      "0.10339999943971634\n",
      "run: 411\n",
      "0.10350000113248825\n",
      "run: 412\n",
      "0.10570000112056732\n",
      "run: 413\n",
      "0.1137000024318695\n",
      "run: 414\n",
      "0.10000000149011612\n",
      "run: 415\n",
      "0.09969999641180038\n",
      "run: 416\n",
      "0.10019999742507935\n",
      "run: 417\n",
      "0.11720000207424164\n",
      "run: 418\n",
      "0.1143999993801117\n",
      "run: 419\n",
      "0.11599999666213989\n",
      "run: 420\n",
      "0.11990000307559967\n",
      "run: 421\n",
      "0.12690000236034393\n",
      "run: 422\n",
      "0.12690000236034393\n",
      "run: 423\n",
      "0.10000000149011612\n",
      "run: 424\n",
      "0.09889999777078629\n",
      "run: 425\n",
      "0.09960000216960907\n",
      "run: 426\n",
      "0.12319999933242798\n",
      "run: 427\n",
      "0.12250000238418579\n",
      "run: 428\n",
      "0.12060000002384186\n",
      "run: 429\n",
      "0.12290000170469284\n",
      "run: 430\n",
      "0.13300000131130219\n",
      "run: 431\n",
      "0.13220000267028809\n",
      "run: 432\n",
      "0.10000000149011612\n",
      "run: 433\n",
      "0.10000000149011612\n",
      "run: 434\n",
      "0.10019999742507935\n",
      "run: 435\n",
      "0.1120000034570694\n",
      "run: 436\n",
      "0.12710000574588776\n",
      "run: 437\n",
      "0.1348000019788742\n",
      "run: 438\n",
      "0.10719999670982361\n",
      "run: 439\n",
      "0.14190000295639038\n",
      "run: 440\n",
      "0.14920000731945038\n",
      "run: 441\n",
      "0.10000000149011612\n",
      "run: 442\n",
      "0.10010000318288803\n",
      "run: 443\n",
      "0.10180000215768814\n",
      "run: 444\n",
      "0.11760000139474869\n",
      "run: 445\n",
      "0.2061000019311905\n",
      "run: 446\n",
      "0.18469999730587006\n",
      "run: 447\n",
      "0.1096000000834465\n",
      "run: 448\n",
      "0.20759999752044678\n",
      "run: 449\n",
      "0.21130000054836273\n",
      "run: 450\n",
      "0.10000000149011612\n",
      "run: 451\n",
      "0.09989999979734421\n",
      "run: 452\n",
      "0.10220000147819519\n",
      "run: 453\n",
      "0.11169999837875366\n",
      "run: 454\n",
      "0.21629999577999115\n",
      "run: 455\n",
      "0.20340000092983246\n",
      "run: 456\n",
      "0.10740000009536743\n",
      "run: 457\n",
      "0.20180000364780426\n",
      "run: 458\n",
      "0.20990000665187836\n",
      "run: 459\n",
      "0.10000000149011612\n",
      "run: 460\n",
      "0.10000000149011612\n",
      "run: 461\n",
      "0.10140000283718109\n",
      "run: 462\n",
      "0.11779999732971191\n",
      "run: 463\n",
      "0.13439999520778656\n",
      "run: 464\n",
      "0.1348000019788742\n",
      "run: 465\n",
      "0.11670000106096268\n",
      "run: 466\n",
      "0.14740000665187836\n",
      "run: 467\n",
      "0.14970000088214874\n",
      "run: 468\n",
      "0.10000000149011612\n",
      "run: 469\n",
      "0.10010000318288803\n",
      "run: 470\n",
      "0.10320000350475311\n",
      "run: 471\n",
      "0.11739999800920486\n",
      "run: 472\n",
      "0.2371000051498413\n",
      "run: 473\n",
      "0.2013999968767166\n",
      "run: 474\n",
      "0.10899999737739563\n",
      "run: 475\n",
      "0.2184000015258789\n",
      "run: 476\n",
      "0.227400004863739\n",
      "run: 477\n",
      "0.10000000149011612\n",
      "run: 478\n",
      "0.10000000149011612\n",
      "run: 479\n",
      "0.10440000146627426\n",
      "run: 480\n",
      "0.11249999701976776\n",
      "run: 481\n",
      "0.23800000548362732\n",
      "run: 482\n",
      "0.21889999508857727\n",
      "run: 483\n",
      "0.10540000349283218\n",
      "run: 484\n",
      "0.21389999985694885\n",
      "run: 485\n",
      "0.2214999943971634\n",
      "run: 486\n",
      "0.10000000149011612\n",
      "run: 487\n",
      "0.10000000149011612\n",
      "run: 488\n",
      "0.10000000149011612\n",
      "run: 489\n",
      "0.10000000149011612\n",
      "run: 490\n",
      "0.10300000011920929\n",
      "run: 491\n",
      "0.10300000011920929\n",
      "run: 492\n",
      "0.10090000182390213\n",
      "run: 493\n",
      "0.10679999738931656\n",
      "run: 494\n",
      "0.10530000180006027\n",
      "run: 495\n",
      "0.10000000149011612\n",
      "run: 496\n",
      "0.10040000081062317\n",
      "run: 497\n",
      "0.10159999877214432\n",
      "run: 498\n",
      "0.10379999876022339\n",
      "run: 499\n",
      "0.1177000030875206\n",
      "run: 500\n",
      "0.11569999903440475\n",
      "run: 501\n",
      "0.10859999805688858\n",
      "run: 502\n",
      "0.11590000241994858\n",
      "run: 503\n",
      "0.1152999997138977\n",
      "run: 504\n",
      "0.10000000149011612\n",
      "run: 505\n",
      "0.10100000351667404\n",
      "run: 506\n",
      "0.10180000215768814\n",
      "run: 507\n",
      "0.1039000004529953\n",
      "run: 508\n",
      "0.11550000309944153\n",
      "run: 509\n",
      "0.11270000040531158\n",
      "run: 510\n",
      "0.10819999873638153\n",
      "run: 511\n",
      "0.11460000276565552\n",
      "run: 512\n",
      "0.11270000040531158\n",
      "run: 513\n",
      "0.10000000149011612\n",
      "run: 514\n",
      "0.10000000149011612\n",
      "run: 515\n",
      "0.09989999979734421\n",
      "run: 516\n",
      "0.10140000283718109\n",
      "run: 517\n",
      "0.10339999943971634\n",
      "run: 518\n",
      "0.10239999741315842\n",
      "run: 519\n",
      "0.10400000214576721\n",
      "run: 520\n",
      "0.10790000110864639\n",
      "run: 521\n",
      "0.10729999840259552\n",
      "run: 522\n",
      "0.10000000149011612\n",
      "run: 523\n",
      "0.09989999979734421\n",
      "run: 524\n",
      "0.09939999878406525\n",
      "run: 525\n",
      "0.10639999806880951\n",
      "run: 526\n",
      "0.1234000027179718\n",
      "run: 527\n",
      "0.1370999962091446\n",
      "run: 528\n",
      "0.10670000314712524\n",
      "run: 529\n",
      "0.12939999997615814\n",
      "run: 530\n",
      "0.1518000066280365\n",
      "run: 531\n",
      "0.10000000149011612\n",
      "run: 532\n",
      "0.09989999979734421\n",
      "run: 533\n",
      "0.09939999878406525\n",
      "run: 534\n",
      "0.10050000250339508\n",
      "run: 535\n",
      "0.13410000503063202\n",
      "run: 536\n",
      "0.15119999647140503\n",
      "run: 537\n",
      "0.11249999701976776\n",
      "run: 538\n",
      "0.13920000195503235\n",
      "run: 539\n",
      "0.164900004863739\n",
      "run: 540\n",
      "0.10000000149011612\n",
      "run: 541\n",
      "0.10000000149011612\n",
      "run: 542\n",
      "0.09989999979734421\n",
      "run: 543\n",
      "0.1039000004529953\n",
      "run: 544\n",
      "0.1096000000834465\n",
      "run: 545\n",
      "0.10740000009536743\n",
      "run: 546\n",
      "0.10809999704360962\n",
      "run: 547\n",
      "0.1152999997138977\n",
      "run: 548\n",
      "0.11659999936819077\n",
      "run: 549\n",
      "0.10000000149011612\n",
      "run: 550\n",
      "0.09989999979734421\n",
      "run: 551\n",
      "0.09939999878406525\n",
      "run: 552\n",
      "0.11249999701976776\n",
      "run: 553\n",
      "0.12479999661445618\n",
      "run: 554\n",
      "0.13590000569820404\n",
      "run: 555\n",
      "0.11089999973773956\n",
      "run: 556\n",
      "0.12860000133514404\n",
      "run: 557\n",
      "0.15139999985694885\n",
      "run: 558\n",
      "0.10000000149011612\n",
      "run: 559\n",
      "0.09989999979734421\n",
      "run: 560\n",
      "0.09969999641180038\n",
      "run: 561\n",
      "0.11069999635219574\n",
      "run: 562\n",
      "0.13570000231266022\n",
      "run: 563\n",
      "0.149399995803833\n",
      "run: 564\n",
      "0.12210000306367874\n",
      "run: 565\n",
      "0.1363999992609024\n",
      "run: 566\n",
      "0.16279999911785126\n",
      "run: 567\n",
      "0.10000000149011612\n",
      "run: 568\n",
      "0.10000000149011612\n",
      "run: 569\n",
      "0.09989999979734421\n",
      "run: 570\n",
      "0.10119999945163727\n",
      "run: 571\n",
      "0.10809999704360962\n",
      "run: 572\n",
      "0.10920000076293945\n",
      "run: 573\n",
      "0.1046999990940094\n",
      "run: 574\n",
      "0.11649999767541885\n",
      "run: 575\n",
      "0.12290000170469284\n",
      "run: 576\n",
      "0.10000000149011612\n",
      "run: 577\n",
      "0.10050000250339508\n",
      "run: 578\n",
      "0.10080000013113022\n",
      "run: 579\n",
      "0.12540000677108765\n",
      "run: 580\n",
      "0.15970000624656677\n",
      "run: 581\n",
      "0.15760000050067902\n",
      "run: 582\n",
      "0.13359999656677246\n",
      "run: 583\n",
      "0.16979999840259552\n",
      "run: 584\n",
      "0.1785999983549118\n",
      "run: 585\n",
      "0.10000000149011612\n",
      "run: 586\n",
      "0.10040000081062317\n",
      "run: 587\n",
      "0.10270000249147415\n",
      "run: 588\n",
      "0.12460000067949295\n",
      "run: 589\n",
      "0.16539999842643738\n",
      "run: 590\n",
      "0.16529999673366547\n",
      "run: 591\n",
      "0.13670000433921814\n",
      "run: 592\n",
      "0.17180000245571136\n",
      "run: 593\n",
      "0.18629999458789825\n",
      "run: 594\n",
      "0.10000000149011612\n",
      "run: 595\n",
      "0.10000000149011612\n",
      "run: 596\n",
      "0.10029999911785126\n",
      "run: 597\n",
      "0.1088000014424324\n",
      "run: 598\n",
      "0.13019999861717224\n",
      "run: 599\n",
      "0.13529999554157257\n",
      "run: 600\n",
      "0.11299999803304672\n",
      "run: 601\n",
      "0.147599995136261\n",
      "run: 602\n",
      "0.164000004529953\n",
      "run: 603\n",
      "0.10000000149011612\n",
      "run: 604\n",
      "0.09969999641180038\n",
      "run: 605\n",
      "0.1006999984383583\n",
      "run: 606\n",
      "0.20319999754428864\n",
      "run: 607\n",
      "0.43849998712539673\n",
      "run: 608\n",
      "0.4991999864578247\n",
      "run: 609\n",
      "0.22939999401569366\n",
      "run: 610\n",
      "0.5501000285148621\n",
      "run: 611\n",
      "0.5929999947547913\n",
      "run: 612\n",
      "0.10000000149011612\n",
      "run: 613\n",
      "0.09950000047683716\n",
      "run: 614\n",
      "0.10180000215768814\n",
      "run: 615\n",
      "0.2152000069618225\n",
      "run: 616\n",
      "0.5156999826431274\n",
      "run: 617\n",
      "0.5715000033378601\n",
      "run: 618\n",
      "0.22759999334812164\n",
      "run: 619\n",
      "0.5958999991416931\n",
      "run: 620\n",
      "0.6351000070571899\n",
      "run: 621\n",
      "0.10000000149011612\n",
      "run: 622\n",
      "0.0997999981045723\n",
      "run: 623\n",
      "0.10019999742507935\n",
      "run: 624\n",
      "0.11479999870061874\n",
      "run: 625\n",
      "0.14650000631809235\n",
      "run: 626\n",
      "0.15209999680519104\n",
      "run: 627\n",
      "0.12210000306367874\n",
      "run: 628\n",
      "0.1678999960422516\n",
      "run: 629\n",
      "0.19210000336170197\n",
      "run: 630\n",
      "0.10000000149011612\n",
      "run: 631\n",
      "0.09969999641180038\n",
      "run: 632\n",
      "0.10180000215768814\n",
      "run: 633\n",
      "0.2176000028848648\n",
      "run: 634\n",
      "0.5254999995231628\n",
      "run: 635\n",
      "0.5737000107765198\n",
      "run: 636\n",
      "0.23319999873638153\n",
      "run: 637\n",
      "0.5978000164031982\n",
      "run: 638\n",
      "0.6316999793052673\n",
      "run: 639\n",
      "0.10000000149011612\n",
      "run: 640\n",
      "0.09969999641180038\n",
      "run: 641\n",
      "0.10270000249147415\n",
      "run: 642\n",
      "0.2231999933719635\n",
      "run: 643\n",
      "0.5746999979019165\n",
      "run: 644\n",
      "0.6183000206947327\n",
      "run: 645\n",
      "0.2257000058889389\n",
      "run: 646\n",
      "0.6151000261306763\n",
      "run: 647\n",
      "0.65420001745224\n",
      "run: 648\n",
      "0.10000000149011612\n",
      "run: 649\n",
      "0.10000000149011612\n",
      "run: 650\n",
      "0.10000000149011612\n",
      "run: 651\n",
      "0.1039000004529953\n",
      "run: 652\n",
      "0.10989999771118164\n",
      "run: 653\n",
      "0.11259999871253967\n",
      "run: 654\n",
      "0.1088000014424324\n",
      "run: 655\n",
      "0.11969999969005585\n",
      "run: 656\n",
      "0.12639999389648438\n",
      "run: 657\n",
      "0.10000000149011612\n",
      "run: 658\n",
      "0.10000000149011612\n",
      "run: 659\n",
      "0.10109999775886536\n",
      "run: 660\n",
      "0.1339000016450882\n",
      "run: 661\n",
      "0.16269999742507935\n",
      "run: 662\n",
      "0.16009999811649323\n",
      "run: 663\n",
      "0.14499999582767487\n",
      "run: 664\n",
      "0.17190000414848328\n",
      "run: 665\n",
      "0.18369999527931213\n",
      "run: 666\n",
      "0.10000000149011612\n",
      "run: 667\n",
      "0.10010000318288803\n",
      "run: 668\n",
      "0.10080000013113022\n",
      "run: 669\n",
      "0.13510000705718994\n",
      "run: 670\n",
      "0.16670000553131104\n",
      "run: 671\n",
      "0.16949999332427979\n",
      "run: 672\n",
      "0.1534000039100647\n",
      "run: 673\n",
      "0.18060000240802765\n",
      "run: 674\n",
      "0.1931000053882599\n",
      "run: 675\n",
      "0.10000000149011612\n",
      "run: 676\n",
      "0.09989999979734421\n",
      "run: 677\n",
      "0.10100000351667404\n",
      "run: 678\n",
      "0.11580000072717667\n",
      "run: 679\n",
      "0.13729999959468842\n",
      "run: 680\n",
      "0.14800000190734863\n",
      "run: 681\n",
      "0.12470000237226486\n",
      "run: 682\n",
      "0.16660000383853912\n",
      "run: 683\n",
      "0.19709999859333038\n",
      "run: 684\n",
      "0.10000000149011612\n",
      "run: 685\n",
      "0.09969999641180038\n",
      "run: 686\n",
      "0.10209999978542328\n",
      "run: 687\n",
      "0.2198999971151352\n",
      "run: 688\n",
      "0.5289999842643738\n",
      "run: 689\n",
      "0.5871000289916992\n",
      "run: 690\n",
      "0.22550000250339508\n",
      "run: 691\n",
      "0.6132000088691711\n",
      "run: 692\n",
      "0.65420001745224\n",
      "run: 693\n",
      "0.10000000149011612\n",
      "run: 694\n",
      "0.10029999911785126\n",
      "run: 695\n",
      "0.10350000113248825\n",
      "run: 696\n",
      "0.22390000522136688\n",
      "run: 697\n",
      "0.5837000012397766\n",
      "run: 698\n",
      "0.6392999887466431\n",
      "run: 699\n",
      "0.2152000069618225\n",
      "run: 700\n",
      "0.6263999938964844\n",
      "run: 701\n",
      "0.6797999739646912\n",
      "run: 702\n",
      "0.10000000149011612\n",
      "run: 703\n",
      "0.09989999979734421\n",
      "run: 704\n",
      "0.1006999984383583\n",
      "run: 705\n",
      "0.1298000067472458\n",
      "run: 706\n",
      "0.1597999930381775\n",
      "run: 707\n",
      "0.17710000276565552\n",
      "run: 708\n",
      "0.1388999968767166\n",
      "run: 709\n",
      "0.1972000002861023\n",
      "run: 710\n",
      "0.23309999704360962\n",
      "run: 711\n",
      "0.10000000149011612\n",
      "run: 712\n",
      "0.09989999979734421\n",
      "run: 713\n",
      "0.1031000018119812\n",
      "run: 714\n",
      "0.22689999639987946\n",
      "run: 715\n",
      "0.5925999879837036\n",
      "run: 716\n",
      "0.6399000287055969\n",
      "run: 717\n",
      "0.22200000286102295\n",
      "run: 718\n",
      "0.6342999935150146\n",
      "run: 719\n",
      "0.6753000020980835\n",
      "run: 720\n",
      "0.10000000149011612\n",
      "run: 721\n",
      "0.10010000318288803\n",
      "run: 722\n",
      "0.10459999740123749\n",
      "run: 723\n",
      "0.2248000055551529\n",
      "run: 724\n",
      "0.6154999732971191\n",
      "run: 725\n",
      "0.671500027179718\n",
      "run: 726\n",
      "0.2070000022649765\n",
      "run: 727\n",
      "0.6277999877929688\n",
      "run: 728\n",
      "0.6862000226974487\n",
      "length of accuracies and permutations. should be equal:\n",
      "729 729\n",
      "(8, 8, 8, 8, 8, 8)\n",
      "new accuracy\n",
      "0.6862000226974487\n",
      "mean used Bits\n",
      "8.0\n"
     ]
    }
   ],
   "source": [
    "##quantize model using function above\n",
    "newModel = quantizeModel(model = model, testImages = testImages, testLabels = testLabels,quantizationMethod = \"linear\", evaluationMethod= \"ratio\", tolerance = 0.4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 6, 9, 10, 11]\n",
      "[1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1]\n",
      "729\n",
      "[(2, 2, 2, 2, 2, 2), (2, 2, 2, 2, 2, 4), (2, 2, 2, 2, 2, 8), (2, 2, 2, 2, 4, 2), (2, 2, 2, 2, 4, 4), (2, 2, 2, 2, 4, 8), (2, 2, 2, 2, 8, 2), (2, 2, 2, 2, 8, 4), (2, 2, 2, 2, 8, 8), (2, 2, 2, 4, 2, 2), (2, 2, 2, 4, 2, 4), (2, 2, 2, 4, 2, 8), (2, 2, 2, 4, 4, 2), (2, 2, 2, 4, 4, 4), (2, 2, 2, 4, 4, 8), (2, 2, 2, 4, 8, 2), (2, 2, 2, 4, 8, 4), (2, 2, 2, 4, 8, 8), (2, 2, 2, 8, 2, 2), (2, 2, 2, 8, 2, 4), (2, 2, 2, 8, 2, 8), (2, 2, 2, 8, 4, 2), (2, 2, 2, 8, 4, 4), (2, 2, 2, 8, 4, 8), (2, 2, 2, 8, 8, 2), (2, 2, 2, 8, 8, 4), (2, 2, 2, 8, 8, 8), (2, 2, 4, 2, 2, 2), (2, 2, 4, 2, 2, 4), (2, 2, 4, 2, 2, 8), (2, 2, 4, 2, 4, 2), (2, 2, 4, 2, 4, 4), (2, 2, 4, 2, 4, 8), (2, 2, 4, 2, 8, 2), (2, 2, 4, 2, 8, 4), (2, 2, 4, 2, 8, 8), (2, 2, 4, 4, 2, 2), (2, 2, 4, 4, 2, 4), (2, 2, 4, 4, 2, 8), (2, 2, 4, 4, 4, 2), (2, 2, 4, 4, 4, 4), (2, 2, 4, 4, 4, 8), (2, 2, 4, 4, 8, 2), (2, 2, 4, 4, 8, 4), (2, 2, 4, 4, 8, 8), (2, 2, 4, 8, 2, 2), (2, 2, 4, 8, 2, 4), (2, 2, 4, 8, 2, 8), (2, 2, 4, 8, 4, 2), (2, 2, 4, 8, 4, 4), (2, 2, 4, 8, 4, 8), (2, 2, 4, 8, 8, 2), (2, 2, 4, 8, 8, 4), (2, 2, 4, 8, 8, 8), (2, 2, 8, 2, 2, 2), (2, 2, 8, 2, 2, 4), (2, 2, 8, 2, 2, 8), (2, 2, 8, 2, 4, 2), (2, 2, 8, 2, 4, 4), (2, 2, 8, 2, 4, 8), (2, 2, 8, 2, 8, 2), (2, 2, 8, 2, 8, 4), (2, 2, 8, 2, 8, 8), (2, 2, 8, 4, 2, 2), (2, 2, 8, 4, 2, 4), (2, 2, 8, 4, 2, 8), (2, 2, 8, 4, 4, 2), (2, 2, 8, 4, 4, 4), (2, 2, 8, 4, 4, 8), (2, 2, 8, 4, 8, 2), (2, 2, 8, 4, 8, 4), (2, 2, 8, 4, 8, 8), (2, 2, 8, 8, 2, 2), (2, 2, 8, 8, 2, 4), (2, 2, 8, 8, 2, 8), (2, 2, 8, 8, 4, 2), (2, 2, 8, 8, 4, 4), (2, 2, 8, 8, 4, 8), (2, 2, 8, 8, 8, 2), (2, 2, 8, 8, 8, 4), (2, 2, 8, 8, 8, 8), (2, 4, 2, 2, 2, 2), (2, 4, 2, 2, 2, 4), (2, 4, 2, 2, 2, 8), (2, 4, 2, 2, 4, 2), (2, 4, 2, 2, 4, 4), (2, 4, 2, 2, 4, 8), (2, 4, 2, 2, 8, 2), (2, 4, 2, 2, 8, 4), (2, 4, 2, 2, 8, 8), (2, 4, 2, 4, 2, 2), (2, 4, 2, 4, 2, 4), (2, 4, 2, 4, 2, 8), (2, 4, 2, 4, 4, 2), (2, 4, 2, 4, 4, 4), (2, 4, 2, 4, 4, 8), (2, 4, 2, 4, 8, 2), (2, 4, 2, 4, 8, 4), (2, 4, 2, 4, 8, 8), (2, 4, 2, 8, 2, 2), (2, 4, 2, 8, 2, 4), (2, 4, 2, 8, 2, 8), (2, 4, 2, 8, 4, 2), (2, 4, 2, 8, 4, 4), (2, 4, 2, 8, 4, 8), (2, 4, 2, 8, 8, 2), (2, 4, 2, 8, 8, 4), (2, 4, 2, 8, 8, 8), (2, 4, 4, 2, 2, 2), (2, 4, 4, 2, 2, 4), (2, 4, 4, 2, 2, 8), (2, 4, 4, 2, 4, 2), (2, 4, 4, 2, 4, 4), (2, 4, 4, 2, 4, 8), (2, 4, 4, 2, 8, 2), (2, 4, 4, 2, 8, 4), (2, 4, 4, 2, 8, 8), (2, 4, 4, 4, 2, 2), (2, 4, 4, 4, 2, 4), (2, 4, 4, 4, 2, 8), (2, 4, 4, 4, 4, 2), (2, 4, 4, 4, 4, 4), (2, 4, 4, 4, 4, 8), (2, 4, 4, 4, 8, 2), (2, 4, 4, 4, 8, 4), (2, 4, 4, 4, 8, 8), (2, 4, 4, 8, 2, 2), (2, 4, 4, 8, 2, 4), (2, 4, 4, 8, 2, 8), (2, 4, 4, 8, 4, 2), (2, 4, 4, 8, 4, 4), (2, 4, 4, 8, 4, 8), (2, 4, 4, 8, 8, 2), (2, 4, 4, 8, 8, 4), (2, 4, 4, 8, 8, 8), (2, 4, 8, 2, 2, 2), (2, 4, 8, 2, 2, 4), (2, 4, 8, 2, 2, 8), (2, 4, 8, 2, 4, 2), (2, 4, 8, 2, 4, 4), (2, 4, 8, 2, 4, 8), (2, 4, 8, 2, 8, 2), (2, 4, 8, 2, 8, 4), (2, 4, 8, 2, 8, 8), (2, 4, 8, 4, 2, 2), (2, 4, 8, 4, 2, 4), (2, 4, 8, 4, 2, 8), (2, 4, 8, 4, 4, 2), (2, 4, 8, 4, 4, 4), (2, 4, 8, 4, 4, 8), (2, 4, 8, 4, 8, 2), (2, 4, 8, 4, 8, 4), (2, 4, 8, 4, 8, 8), (2, 4, 8, 8, 2, 2), (2, 4, 8, 8, 2, 4), (2, 4, 8, 8, 2, 8), (2, 4, 8, 8, 4, 2), (2, 4, 8, 8, 4, 4), (2, 4, 8, 8, 4, 8), (2, 4, 8, 8, 8, 2), (2, 4, 8, 8, 8, 4), (2, 4, 8, 8, 8, 8), (2, 8, 2, 2, 2, 2), (2, 8, 2, 2, 2, 4), (2, 8, 2, 2, 2, 8), (2, 8, 2, 2, 4, 2), (2, 8, 2, 2, 4, 4), (2, 8, 2, 2, 4, 8), (2, 8, 2, 2, 8, 2), (2, 8, 2, 2, 8, 4), (2, 8, 2, 2, 8, 8), (2, 8, 2, 4, 2, 2), (2, 8, 2, 4, 2, 4), (2, 8, 2, 4, 2, 8), (2, 8, 2, 4, 4, 2), (2, 8, 2, 4, 4, 4), (2, 8, 2, 4, 4, 8), (2, 8, 2, 4, 8, 2), (2, 8, 2, 4, 8, 4), (2, 8, 2, 4, 8, 8), (2, 8, 2, 8, 2, 2), (2, 8, 2, 8, 2, 4), (2, 8, 2, 8, 2, 8), (2, 8, 2, 8, 4, 2), (2, 8, 2, 8, 4, 4), (2, 8, 2, 8, 4, 8), (2, 8, 2, 8, 8, 2), (2, 8, 2, 8, 8, 4), (2, 8, 2, 8, 8, 8), (2, 8, 4, 2, 2, 2), (2, 8, 4, 2, 2, 4), (2, 8, 4, 2, 2, 8), (2, 8, 4, 2, 4, 2), (2, 8, 4, 2, 4, 4), (2, 8, 4, 2, 4, 8), (2, 8, 4, 2, 8, 2), (2, 8, 4, 2, 8, 4), (2, 8, 4, 2, 8, 8), (2, 8, 4, 4, 2, 2), (2, 8, 4, 4, 2, 4), (2, 8, 4, 4, 2, 8), (2, 8, 4, 4, 4, 2), (2, 8, 4, 4, 4, 4), (2, 8, 4, 4, 4, 8), (2, 8, 4, 4, 8, 2), (2, 8, 4, 4, 8, 4), (2, 8, 4, 4, 8, 8), (2, 8, 4, 8, 2, 2), (2, 8, 4, 8, 2, 4), (2, 8, 4, 8, 2, 8), (2, 8, 4, 8, 4, 2), (2, 8, 4, 8, 4, 4), (2, 8, 4, 8, 4, 8), (2, 8, 4, 8, 8, 2), (2, 8, 4, 8, 8, 4), (2, 8, 4, 8, 8, 8), (2, 8, 8, 2, 2, 2), (2, 8, 8, 2, 2, 4), (2, 8, 8, 2, 2, 8), (2, 8, 8, 2, 4, 2), (2, 8, 8, 2, 4, 4), (2, 8, 8, 2, 4, 8), (2, 8, 8, 2, 8, 2), (2, 8, 8, 2, 8, 4), (2, 8, 8, 2, 8, 8), (2, 8, 8, 4, 2, 2), (2, 8, 8, 4, 2, 4), (2, 8, 8, 4, 2, 8), (2, 8, 8, 4, 4, 2), (2, 8, 8, 4, 4, 4), (2, 8, 8, 4, 4, 8), (2, 8, 8, 4, 8, 2), (2, 8, 8, 4, 8, 4), (2, 8, 8, 4, 8, 8), (2, 8, 8, 8, 2, 2), (2, 8, 8, 8, 2, 4), (2, 8, 8, 8, 2, 8), (2, 8, 8, 8, 4, 2), (2, 8, 8, 8, 4, 4), (2, 8, 8, 8, 4, 8), (2, 8, 8, 8, 8, 2), (2, 8, 8, 8, 8, 4), (2, 8, 8, 8, 8, 8), (4, 2, 2, 2, 2, 2), (4, 2, 2, 2, 2, 4), (4, 2, 2, 2, 2, 8), (4, 2, 2, 2, 4, 2), (4, 2, 2, 2, 4, 4), (4, 2, 2, 2, 4, 8), (4, 2, 2, 2, 8, 2), (4, 2, 2, 2, 8, 4), (4, 2, 2, 2, 8, 8), (4, 2, 2, 4, 2, 2), (4, 2, 2, 4, 2, 4), (4, 2, 2, 4, 2, 8), (4, 2, 2, 4, 4, 2), (4, 2, 2, 4, 4, 4), (4, 2, 2, 4, 4, 8), (4, 2, 2, 4, 8, 2), (4, 2, 2, 4, 8, 4), (4, 2, 2, 4, 8, 8), (4, 2, 2, 8, 2, 2), (4, 2, 2, 8, 2, 4), (4, 2, 2, 8, 2, 8), (4, 2, 2, 8, 4, 2), (4, 2, 2, 8, 4, 4), (4, 2, 2, 8, 4, 8), (4, 2, 2, 8, 8, 2), (4, 2, 2, 8, 8, 4), (4, 2, 2, 8, 8, 8), (4, 2, 4, 2, 2, 2), (4, 2, 4, 2, 2, 4), (4, 2, 4, 2, 2, 8), (4, 2, 4, 2, 4, 2), (4, 2, 4, 2, 4, 4), (4, 2, 4, 2, 4, 8), (4, 2, 4, 2, 8, 2), (4, 2, 4, 2, 8, 4), (4, 2, 4, 2, 8, 8), (4, 2, 4, 4, 2, 2), (4, 2, 4, 4, 2, 4), (4, 2, 4, 4, 2, 8), (4, 2, 4, 4, 4, 2), (4, 2, 4, 4, 4, 4), (4, 2, 4, 4, 4, 8), (4, 2, 4, 4, 8, 2), (4, 2, 4, 4, 8, 4), (4, 2, 4, 4, 8, 8), (4, 2, 4, 8, 2, 2), (4, 2, 4, 8, 2, 4), (4, 2, 4, 8, 2, 8), (4, 2, 4, 8, 4, 2), (4, 2, 4, 8, 4, 4), (4, 2, 4, 8, 4, 8), (4, 2, 4, 8, 8, 2), (4, 2, 4, 8, 8, 4), (4, 2, 4, 8, 8, 8), (4, 2, 8, 2, 2, 2), (4, 2, 8, 2, 2, 4), (4, 2, 8, 2, 2, 8), (4, 2, 8, 2, 4, 2), (4, 2, 8, 2, 4, 4), (4, 2, 8, 2, 4, 8), (4, 2, 8, 2, 8, 2), (4, 2, 8, 2, 8, 4), (4, 2, 8, 2, 8, 8), (4, 2, 8, 4, 2, 2), (4, 2, 8, 4, 2, 4), (4, 2, 8, 4, 2, 8), (4, 2, 8, 4, 4, 2), (4, 2, 8, 4, 4, 4), (4, 2, 8, 4, 4, 8), (4, 2, 8, 4, 8, 2), (4, 2, 8, 4, 8, 4), (4, 2, 8, 4, 8, 8), (4, 2, 8, 8, 2, 2), (4, 2, 8, 8, 2, 4), (4, 2, 8, 8, 2, 8), (4, 2, 8, 8, 4, 2), (4, 2, 8, 8, 4, 4), (4, 2, 8, 8, 4, 8), (4, 2, 8, 8, 8, 2), (4, 2, 8, 8, 8, 4), (4, 2, 8, 8, 8, 8), (4, 4, 2, 2, 2, 2), (4, 4, 2, 2, 2, 4), (4, 4, 2, 2, 2, 8), (4, 4, 2, 2, 4, 2), (4, 4, 2, 2, 4, 4), (4, 4, 2, 2, 4, 8), (4, 4, 2, 2, 8, 2), (4, 4, 2, 2, 8, 4), (4, 4, 2, 2, 8, 8), (4, 4, 2, 4, 2, 2), (4, 4, 2, 4, 2, 4), (4, 4, 2, 4, 2, 8), (4, 4, 2, 4, 4, 2), (4, 4, 2, 4, 4, 4), (4, 4, 2, 4, 4, 8), (4, 4, 2, 4, 8, 2), (4, 4, 2, 4, 8, 4), (4, 4, 2, 4, 8, 8), (4, 4, 2, 8, 2, 2), (4, 4, 2, 8, 2, 4), (4, 4, 2, 8, 2, 8), (4, 4, 2, 8, 4, 2), (4, 4, 2, 8, 4, 4), (4, 4, 2, 8, 4, 8), (4, 4, 2, 8, 8, 2), (4, 4, 2, 8, 8, 4), (4, 4, 2, 8, 8, 8), (4, 4, 4, 2, 2, 2), (4, 4, 4, 2, 2, 4), (4, 4, 4, 2, 2, 8), (4, 4, 4, 2, 4, 2), (4, 4, 4, 2, 4, 4), (4, 4, 4, 2, 4, 8), (4, 4, 4, 2, 8, 2), (4, 4, 4, 2, 8, 4), (4, 4, 4, 2, 8, 8), (4, 4, 4, 4, 2, 2), (4, 4, 4, 4, 2, 4), (4, 4, 4, 4, 2, 8), (4, 4, 4, 4, 4, 2), (4, 4, 4, 4, 4, 4), (4, 4, 4, 4, 4, 8), (4, 4, 4, 4, 8, 2), (4, 4, 4, 4, 8, 4), (4, 4, 4, 4, 8, 8), (4, 4, 4, 8, 2, 2), (4, 4, 4, 8, 2, 4), (4, 4, 4, 8, 2, 8), (4, 4, 4, 8, 4, 2), (4, 4, 4, 8, 4, 4), (4, 4, 4, 8, 4, 8), (4, 4, 4, 8, 8, 2), (4, 4, 4, 8, 8, 4), (4, 4, 4, 8, 8, 8), (4, 4, 8, 2, 2, 2), (4, 4, 8, 2, 2, 4), (4, 4, 8, 2, 2, 8), (4, 4, 8, 2, 4, 2), (4, 4, 8, 2, 4, 4), (4, 4, 8, 2, 4, 8), (4, 4, 8, 2, 8, 2), (4, 4, 8, 2, 8, 4), (4, 4, 8, 2, 8, 8), (4, 4, 8, 4, 2, 2), (4, 4, 8, 4, 2, 4), (4, 4, 8, 4, 2, 8), (4, 4, 8, 4, 4, 2), (4, 4, 8, 4, 4, 4), (4, 4, 8, 4, 4, 8), (4, 4, 8, 4, 8, 2), (4, 4, 8, 4, 8, 4), (4, 4, 8, 4, 8, 8), (4, 4, 8, 8, 2, 2), (4, 4, 8, 8, 2, 4), (4, 4, 8, 8, 2, 8), (4, 4, 8, 8, 4, 2), (4, 4, 8, 8, 4, 4), (4, 4, 8, 8, 4, 8), (4, 4, 8, 8, 8, 2), (4, 4, 8, 8, 8, 4), (4, 4, 8, 8, 8, 8), (4, 8, 2, 2, 2, 2), (4, 8, 2, 2, 2, 4), (4, 8, 2, 2, 2, 8), (4, 8, 2, 2, 4, 2), (4, 8, 2, 2, 4, 4), (4, 8, 2, 2, 4, 8), (4, 8, 2, 2, 8, 2), (4, 8, 2, 2, 8, 4), (4, 8, 2, 2, 8, 8), (4, 8, 2, 4, 2, 2), (4, 8, 2, 4, 2, 4), (4, 8, 2, 4, 2, 8), (4, 8, 2, 4, 4, 2), (4, 8, 2, 4, 4, 4), (4, 8, 2, 4, 4, 8), (4, 8, 2, 4, 8, 2), (4, 8, 2, 4, 8, 4), (4, 8, 2, 4, 8, 8), (4, 8, 2, 8, 2, 2), (4, 8, 2, 8, 2, 4), (4, 8, 2, 8, 2, 8), (4, 8, 2, 8, 4, 2), (4, 8, 2, 8, 4, 4), (4, 8, 2, 8, 4, 8), (4, 8, 2, 8, 8, 2), (4, 8, 2, 8, 8, 4), (4, 8, 2, 8, 8, 8), (4, 8, 4, 2, 2, 2), (4, 8, 4, 2, 2, 4), (4, 8, 4, 2, 2, 8), (4, 8, 4, 2, 4, 2), (4, 8, 4, 2, 4, 4), (4, 8, 4, 2, 4, 8), (4, 8, 4, 2, 8, 2), (4, 8, 4, 2, 8, 4), (4, 8, 4, 2, 8, 8), (4, 8, 4, 4, 2, 2), (4, 8, 4, 4, 2, 4), (4, 8, 4, 4, 2, 8), (4, 8, 4, 4, 4, 2), (4, 8, 4, 4, 4, 4), (4, 8, 4, 4, 4, 8), (4, 8, 4, 4, 8, 2), (4, 8, 4, 4, 8, 4), (4, 8, 4, 4, 8, 8), (4, 8, 4, 8, 2, 2), (4, 8, 4, 8, 2, 4), (4, 8, 4, 8, 2, 8), (4, 8, 4, 8, 4, 2), (4, 8, 4, 8, 4, 4), (4, 8, 4, 8, 4, 8), (4, 8, 4, 8, 8, 2), (4, 8, 4, 8, 8, 4), (4, 8, 4, 8, 8, 8), (4, 8, 8, 2, 2, 2), (4, 8, 8, 2, 2, 4), (4, 8, 8, 2, 2, 8), (4, 8, 8, 2, 4, 2), (4, 8, 8, 2, 4, 4), (4, 8, 8, 2, 4, 8), (4, 8, 8, 2, 8, 2), (4, 8, 8, 2, 8, 4), (4, 8, 8, 2, 8, 8), (4, 8, 8, 4, 2, 2), (4, 8, 8, 4, 2, 4), (4, 8, 8, 4, 2, 8), (4, 8, 8, 4, 4, 2), (4, 8, 8, 4, 4, 4), (4, 8, 8, 4, 4, 8), (4, 8, 8, 4, 8, 2), (4, 8, 8, 4, 8, 4), (4, 8, 8, 4, 8, 8), (4, 8, 8, 8, 2, 2), (4, 8, 8, 8, 2, 4), (4, 8, 8, 8, 2, 8), (4, 8, 8, 8, 4, 2), (4, 8, 8, 8, 4, 4), (4, 8, 8, 8, 4, 8), (4, 8, 8, 8, 8, 2), (4, 8, 8, 8, 8, 4), (4, 8, 8, 8, 8, 8), (8, 2, 2, 2, 2, 2), (8, 2, 2, 2, 2, 4), (8, 2, 2, 2, 2, 8), (8, 2, 2, 2, 4, 2), (8, 2, 2, 2, 4, 4), (8, 2, 2, 2, 4, 8), (8, 2, 2, 2, 8, 2), (8, 2, 2, 2, 8, 4), (8, 2, 2, 2, 8, 8), (8, 2, 2, 4, 2, 2), (8, 2, 2, 4, 2, 4), (8, 2, 2, 4, 2, 8), (8, 2, 2, 4, 4, 2), (8, 2, 2, 4, 4, 4), (8, 2, 2, 4, 4, 8), (8, 2, 2, 4, 8, 2), (8, 2, 2, 4, 8, 4), (8, 2, 2, 4, 8, 8), (8, 2, 2, 8, 2, 2), (8, 2, 2, 8, 2, 4), (8, 2, 2, 8, 2, 8), (8, 2, 2, 8, 4, 2), (8, 2, 2, 8, 4, 4), (8, 2, 2, 8, 4, 8), (8, 2, 2, 8, 8, 2), (8, 2, 2, 8, 8, 4), (8, 2, 2, 8, 8, 8), (8, 2, 4, 2, 2, 2), (8, 2, 4, 2, 2, 4), (8, 2, 4, 2, 2, 8), (8, 2, 4, 2, 4, 2), (8, 2, 4, 2, 4, 4), (8, 2, 4, 2, 4, 8), (8, 2, 4, 2, 8, 2), (8, 2, 4, 2, 8, 4), (8, 2, 4, 2, 8, 8), (8, 2, 4, 4, 2, 2), (8, 2, 4, 4, 2, 4), (8, 2, 4, 4, 2, 8), (8, 2, 4, 4, 4, 2), (8, 2, 4, 4, 4, 4), (8, 2, 4, 4, 4, 8), (8, 2, 4, 4, 8, 2), (8, 2, 4, 4, 8, 4), (8, 2, 4, 4, 8, 8), (8, 2, 4, 8, 2, 2), (8, 2, 4, 8, 2, 4), (8, 2, 4, 8, 2, 8), (8, 2, 4, 8, 4, 2), (8, 2, 4, 8, 4, 4), (8, 2, 4, 8, 4, 8), (8, 2, 4, 8, 8, 2), (8, 2, 4, 8, 8, 4), (8, 2, 4, 8, 8, 8), (8, 2, 8, 2, 2, 2), (8, 2, 8, 2, 2, 4), (8, 2, 8, 2, 2, 8), (8, 2, 8, 2, 4, 2), (8, 2, 8, 2, 4, 4), (8, 2, 8, 2, 4, 8), (8, 2, 8, 2, 8, 2), (8, 2, 8, 2, 8, 4), (8, 2, 8, 2, 8, 8), (8, 2, 8, 4, 2, 2), (8, 2, 8, 4, 2, 4), (8, 2, 8, 4, 2, 8), (8, 2, 8, 4, 4, 2), (8, 2, 8, 4, 4, 4), (8, 2, 8, 4, 4, 8), (8, 2, 8, 4, 8, 2), (8, 2, 8, 4, 8, 4), (8, 2, 8, 4, 8, 8), (8, 2, 8, 8, 2, 2), (8, 2, 8, 8, 2, 4), (8, 2, 8, 8, 2, 8), (8, 2, 8, 8, 4, 2), (8, 2, 8, 8, 4, 4), (8, 2, 8, 8, 4, 8), (8, 2, 8, 8, 8, 2), (8, 2, 8, 8, 8, 4), (8, 2, 8, 8, 8, 8), (8, 4, 2, 2, 2, 2), (8, 4, 2, 2, 2, 4), (8, 4, 2, 2, 2, 8), (8, 4, 2, 2, 4, 2), (8, 4, 2, 2, 4, 4), (8, 4, 2, 2, 4, 8), (8, 4, 2, 2, 8, 2), (8, 4, 2, 2, 8, 4), (8, 4, 2, 2, 8, 8), (8, 4, 2, 4, 2, 2), (8, 4, 2, 4, 2, 4), (8, 4, 2, 4, 2, 8), (8, 4, 2, 4, 4, 2), (8, 4, 2, 4, 4, 4), (8, 4, 2, 4, 4, 8), (8, 4, 2, 4, 8, 2), (8, 4, 2, 4, 8, 4), (8, 4, 2, 4, 8, 8), (8, 4, 2, 8, 2, 2), (8, 4, 2, 8, 2, 4), (8, 4, 2, 8, 2, 8), (8, 4, 2, 8, 4, 2), (8, 4, 2, 8, 4, 4), (8, 4, 2, 8, 4, 8), (8, 4, 2, 8, 8, 2), (8, 4, 2, 8, 8, 4), (8, 4, 2, 8, 8, 8), (8, 4, 4, 2, 2, 2), (8, 4, 4, 2, 2, 4), (8, 4, 4, 2, 2, 8), (8, 4, 4, 2, 4, 2), (8, 4, 4, 2, 4, 4), (8, 4, 4, 2, 4, 8), (8, 4, 4, 2, 8, 2), (8, 4, 4, 2, 8, 4), (8, 4, 4, 2, 8, 8), (8, 4, 4, 4, 2, 2), (8, 4, 4, 4, 2, 4), (8, 4, 4, 4, 2, 8), (8, 4, 4, 4, 4, 2), (8, 4, 4, 4, 4, 4), (8, 4, 4, 4, 4, 8), (8, 4, 4, 4, 8, 2), (8, 4, 4, 4, 8, 4), (8, 4, 4, 4, 8, 8), (8, 4, 4, 8, 2, 2), (8, 4, 4, 8, 2, 4), (8, 4, 4, 8, 2, 8), (8, 4, 4, 8, 4, 2), (8, 4, 4, 8, 4, 4), (8, 4, 4, 8, 4, 8), (8, 4, 4, 8, 8, 2), (8, 4, 4, 8, 8, 4), (8, 4, 4, 8, 8, 8), (8, 4, 8, 2, 2, 2), (8, 4, 8, 2, 2, 4), (8, 4, 8, 2, 2, 8), (8, 4, 8, 2, 4, 2), (8, 4, 8, 2, 4, 4), (8, 4, 8, 2, 4, 8), (8, 4, 8, 2, 8, 2), (8, 4, 8, 2, 8, 4), (8, 4, 8, 2, 8, 8), (8, 4, 8, 4, 2, 2), (8, 4, 8, 4, 2, 4), (8, 4, 8, 4, 2, 8), (8, 4, 8, 4, 4, 2), (8, 4, 8, 4, 4, 4), (8, 4, 8, 4, 4, 8), (8, 4, 8, 4, 8, 2), (8, 4, 8, 4, 8, 4), (8, 4, 8, 4, 8, 8), (8, 4, 8, 8, 2, 2), (8, 4, 8, 8, 2, 4), (8, 4, 8, 8, 2, 8), (8, 4, 8, 8, 4, 2), (8, 4, 8, 8, 4, 4), (8, 4, 8, 8, 4, 8), (8, 4, 8, 8, 8, 2), (8, 4, 8, 8, 8, 4), (8, 4, 8, 8, 8, 8), (8, 8, 2, 2, 2, 2), (8, 8, 2, 2, 2, 4), (8, 8, 2, 2, 2, 8), (8, 8, 2, 2, 4, 2), (8, 8, 2, 2, 4, 4), (8, 8, 2, 2, 4, 8), (8, 8, 2, 2, 8, 2), (8, 8, 2, 2, 8, 4), (8, 8, 2, 2, 8, 8), (8, 8, 2, 4, 2, 2), (8, 8, 2, 4, 2, 4), (8, 8, 2, 4, 2, 8), (8, 8, 2, 4, 4, 2), (8, 8, 2, 4, 4, 4), (8, 8, 2, 4, 4, 8), (8, 8, 2, 4, 8, 2), (8, 8, 2, 4, 8, 4), (8, 8, 2, 4, 8, 8), (8, 8, 2, 8, 2, 2), (8, 8, 2, 8, 2, 4), (8, 8, 2, 8, 2, 8), (8, 8, 2, 8, 4, 2), (8, 8, 2, 8, 4, 4), (8, 8, 2, 8, 4, 8), (8, 8, 2, 8, 8, 2), (8, 8, 2, 8, 8, 4), (8, 8, 2, 8, 8, 8), (8, 8, 4, 2, 2, 2), (8, 8, 4, 2, 2, 4), (8, 8, 4, 2, 2, 8), (8, 8, 4, 2, 4, 2), (8, 8, 4, 2, 4, 4), (8, 8, 4, 2, 4, 8), (8, 8, 4, 2, 8, 2), (8, 8, 4, 2, 8, 4), (8, 8, 4, 2, 8, 8), (8, 8, 4, 4, 2, 2), (8, 8, 4, 4, 2, 4), (8, 8, 4, 4, 2, 8), (8, 8, 4, 4, 4, 2), (8, 8, 4, 4, 4, 4), (8, 8, 4, 4, 4, 8), (8, 8, 4, 4, 8, 2), (8, 8, 4, 4, 8, 4), (8, 8, 4, 4, 8, 8), (8, 8, 4, 8, 2, 2), (8, 8, 4, 8, 2, 4), (8, 8, 4, 8, 2, 8), (8, 8, 4, 8, 4, 2), (8, 8, 4, 8, 4, 4), (8, 8, 4, 8, 4, 8), (8, 8, 4, 8, 8, 2), (8, 8, 4, 8, 8, 4), (8, 8, 4, 8, 8, 8), (8, 8, 8, 2, 2, 2), (8, 8, 8, 2, 2, 4), (8, 8, 8, 2, 2, 8), (8, 8, 8, 2, 4, 2), (8, 8, 8, 2, 4, 4), (8, 8, 8, 2, 4, 8), (8, 8, 8, 2, 8, 2), (8, 8, 8, 2, 8, 4), (8, 8, 8, 2, 8, 8), (8, 8, 8, 4, 2, 2), (8, 8, 8, 4, 2, 4), (8, 8, 8, 4, 2, 8), (8, 8, 8, 4, 4, 2), (8, 8, 8, 4, 4, 4), (8, 8, 8, 4, 4, 8), (8, 8, 8, 4, 8, 2), (8, 8, 8, 4, 8, 4), (8, 8, 8, 4, 8, 8), (8, 8, 8, 8, 2, 2), (8, 8, 8, 8, 2, 4), (8, 8, 8, 8, 2, 8), (8, 8, 8, 8, 4, 2), (8, 8, 8, 8, 4, 4), (8, 8, 8, 8, 4, 8), (8, 8, 8, 8, 8, 2), (8, 8, 8, 8, 8, 4), (8, 8, 8, 8, 8, 8)]\n",
      "run: 0\n",
      "0.10000000149011612\n",
      "run: 1\n",
      "0.10199999809265137\n",
      "run: 2\n",
      "0.10199999809265137\n",
      "run: 3\n",
      "0.10000000149011612\n",
      "run: 4\n",
      "0.1096000000834465\n",
      "run: 5\n",
      "0.1096000000834465\n",
      "run: 6\n",
      "0.10000000149011612\n",
      "run: 7\n",
      "0.1096000000834465\n",
      "run: 8\n",
      "0.1096000000834465\n",
      "run: 9\n",
      "0.10019999742507935\n",
      "run: 10\n",
      "0.10429999977350235\n",
      "run: 11\n",
      "0.10429999977350235\n",
      "run: 12\n",
      "0.10040000081062317\n",
      "run: 13\n",
      "0.1152999997138977\n",
      "run: 14\n",
      "0.1152999997138977\n",
      "run: 15\n",
      "0.10040000081062317\n",
      "run: 16\n",
      "0.1152999997138977\n",
      "run: 17\n",
      "0.1152999997138977\n",
      "run: 18\n",
      "0.10019999742507935\n",
      "run: 19\n",
      "0.10429999977350235\n",
      "run: 20\n",
      "0.10429999977350235\n",
      "run: 21\n",
      "0.10040000081062317\n",
      "run: 22\n",
      "0.1152999997138977\n",
      "run: 23\n",
      "0.1152999997138977\n",
      "run: 24\n",
      "0.10040000081062317\n",
      "run: 25\n",
      "0.1152999997138977\n",
      "run: 26\n",
      "0.1152999997138977\n",
      "run: 27\n",
      "0.10010000318288803\n",
      "run: 28\n",
      "0.10790000110864639\n",
      "run: 29\n",
      "0.10790000110864639\n",
      "run: 30\n",
      "0.10100000351667404\n",
      "run: 31\n",
      "0.1128000020980835\n",
      "run: 32\n",
      "0.1128000020980835\n",
      "run: 33\n",
      "0.10100000351667404\n",
      "run: 34\n",
      "0.1128000020980835\n",
      "run: 35\n",
      "0.1128000020980835\n",
      "run: 36\n",
      "0.1014999970793724\n",
      "run: 37\n",
      "0.10949999839067459\n",
      "run: 38\n",
      "0.10949999839067459\n",
      "run: 39\n",
      "0.11190000176429749\n",
      "run: 40\n",
      "0.10729999840259552\n",
      "run: 41\n",
      "0.10729999840259552\n",
      "run: 42\n",
      "0.11190000176429749\n",
      "run: 43\n",
      "0.10729999840259552\n",
      "run: 44\n",
      "0.10729999840259552\n",
      "run: 45\n",
      "0.1014999970793724\n",
      "run: 46\n",
      "0.10949999839067459\n",
      "run: 47\n",
      "0.10949999839067459\n",
      "run: 48\n",
      "0.11190000176429749\n",
      "run: 49\n",
      "0.10729999840259552\n",
      "run: 50\n",
      "0.10729999840259552\n",
      "run: 51\n",
      "0.11190000176429749\n",
      "run: 52\n",
      "0.10729999840259552\n",
      "run: 53\n",
      "0.10729999840259552\n",
      "run: 54\n",
      "0.10010000318288803\n",
      "run: 55\n",
      "0.10790000110864639\n",
      "run: 56\n",
      "0.10790000110864639\n",
      "run: 57\n",
      "0.10100000351667404\n",
      "run: 58\n",
      "0.1128000020980835\n",
      "run: 59\n",
      "0.1128000020980835\n",
      "run: 60\n",
      "0.10100000351667404\n",
      "run: 61\n",
      "0.1128000020980835\n",
      "run: 62\n",
      "0.1128000020980835\n",
      "run: 63\n",
      "0.1014999970793724\n",
      "run: 64\n",
      "0.10949999839067459\n",
      "run: 65\n",
      "0.10949999839067459\n",
      "run: 66\n",
      "0.11190000176429749\n",
      "run: 67\n",
      "0.10729999840259552\n",
      "run: 68\n",
      "0.10729999840259552\n",
      "run: 69\n",
      "0.11190000176429749\n",
      "run: 70\n",
      "0.10729999840259552\n",
      "run: 71\n",
      "0.10729999840259552\n",
      "run: 72\n",
      "0.1014999970793724\n",
      "run: 73\n",
      "0.10949999839067459\n",
      "run: 74\n",
      "0.10949999839067459\n",
      "run: 75\n",
      "0.11190000176429749\n",
      "run: 76\n",
      "0.10729999840259552\n",
      "run: 77\n",
      "0.10729999840259552\n",
      "run: 78\n",
      "0.11190000176429749\n",
      "run: 79\n",
      "0.10729999840259552\n",
      "run: 80\n",
      "0.10729999840259552\n",
      "run: 81\n",
      "0.10010000318288803\n",
      "run: 82\n",
      "0.10559999942779541\n",
      "run: 83\n",
      "0.10559999942779541\n",
      "run: 84\n",
      "0.1006999984383583\n",
      "run: 85\n",
      "0.1185000017285347\n",
      "run: 86\n",
      "0.1185000017285347\n",
      "run: 87\n",
      "0.1006999984383583\n",
      "run: 88\n",
      "0.1185000017285347\n",
      "run: 89\n",
      "0.1185000017285347\n",
      "run: 90\n",
      "0.1136000007390976\n",
      "run: 91\n",
      "0.11949999630451202\n",
      "run: 92\n",
      "0.11949999630451202\n",
      "run: 93\n",
      "0.12839999794960022\n",
      "run: 94\n",
      "0.13609999418258667\n",
      "run: 95\n",
      "0.13609999418258667\n",
      "run: 96\n",
      "0.12839999794960022\n",
      "run: 97\n",
      "0.13609999418258667\n",
      "run: 98\n",
      "0.13609999418258667\n",
      "run: 99\n",
      "0.1136000007390976\n",
      "run: 100\n",
      "0.11949999630451202\n",
      "run: 101\n",
      "0.11949999630451202\n",
      "run: 102\n",
      "0.12839999794960022\n",
      "run: 103\n",
      "0.13609999418258667\n",
      "run: 104\n",
      "0.13609999418258667\n",
      "run: 105\n",
      "0.12839999794960022\n",
      "run: 106\n",
      "0.13609999418258667\n",
      "run: 107\n",
      "0.13609999418258667\n",
      "run: 108\n",
      "0.10170000046491623\n",
      "run: 109\n",
      "0.1160999983549118\n",
      "run: 110\n",
      "0.1160999983549118\n",
      "run: 111\n",
      "0.11490000039339066\n",
      "run: 112\n",
      "0.1429000049829483\n",
      "run: 113\n",
      "0.1429000049829483\n",
      "run: 114\n",
      "0.11490000039339066\n",
      "run: 115\n",
      "0.1429000049829483\n",
      "run: 116\n",
      "0.1429000049829483\n",
      "run: 117\n",
      "0.12349999696016312\n",
      "run: 118\n",
      "0.12559999525547028\n",
      "run: 119\n",
      "0.12559999525547028\n",
      "run: 120\n",
      "0.14900000393390656\n",
      "run: 121\n",
      "0.14869999885559082\n",
      "run: 122\n",
      "0.14869999885559082\n",
      "run: 123\n",
      "0.14900000393390656\n",
      "run: 124\n",
      "0.14869999885559082\n",
      "run: 125\n",
      "0.14869999885559082\n",
      "run: 126\n",
      "0.12349999696016312\n",
      "run: 127\n",
      "0.12559999525547028\n",
      "run: 128\n",
      "0.12559999525547028\n",
      "run: 129\n",
      "0.14900000393390656\n",
      "run: 130\n",
      "0.14869999885559082\n",
      "run: 131\n",
      "0.14869999885559082\n",
      "run: 132\n",
      "0.14900000393390656\n",
      "run: 133\n",
      "0.14869999885559082\n",
      "run: 134\n",
      "0.14869999885559082\n",
      "run: 135\n",
      "0.10170000046491623\n",
      "run: 136\n",
      "0.1160999983549118\n",
      "run: 137\n",
      "0.1160999983549118\n",
      "run: 138\n",
      "0.11490000039339066\n",
      "run: 139\n",
      "0.1429000049829483\n",
      "run: 140\n",
      "0.1429000049829483\n",
      "run: 141\n",
      "0.11490000039339066\n",
      "run: 142\n",
      "0.1429000049829483\n",
      "run: 143\n",
      "0.1429000049829483\n",
      "run: 144\n",
      "0.12349999696016312\n",
      "run: 145\n",
      "0.12559999525547028\n",
      "run: 146\n",
      "0.12559999525547028\n",
      "run: 147\n",
      "0.14900000393390656\n",
      "run: 148\n",
      "0.14869999885559082\n",
      "run: 149\n",
      "0.14869999885559082\n",
      "run: 150\n",
      "0.14900000393390656\n",
      "run: 151\n",
      "0.14869999885559082\n",
      "run: 152\n",
      "0.14869999885559082\n",
      "run: 153\n",
      "0.12349999696016312\n",
      "run: 154\n",
      "0.12559999525547028\n",
      "run: 155\n",
      "0.12559999525547028\n",
      "run: 156\n",
      "0.14900000393390656\n",
      "run: 157\n",
      "0.14869999885559082\n",
      "run: 158\n",
      "0.14869999885559082\n",
      "run: 159\n",
      "0.14900000393390656\n",
      "run: 160\n",
      "0.14869999885559082\n",
      "run: 161\n",
      "0.14869999885559082\n",
      "run: 162\n",
      "0.10010000318288803\n",
      "run: 163\n",
      "0.10559999942779541\n",
      "run: 164\n",
      "0.10559999942779541\n",
      "run: 165\n",
      "0.1006999984383583\n",
      "run: 166\n",
      "0.1185000017285347\n",
      "run: 167\n",
      "0.1185000017285347\n",
      "run: 168\n",
      "0.1006999984383583\n",
      "run: 169\n",
      "0.1185000017285347\n",
      "run: 170\n",
      "0.1185000017285347\n",
      "run: 171\n",
      "0.1136000007390976\n",
      "run: 172\n",
      "0.11949999630451202\n",
      "run: 173\n",
      "0.11949999630451202\n",
      "run: 174\n",
      "0.12839999794960022\n",
      "run: 175\n",
      "0.13609999418258667\n",
      "run: 176\n",
      "0.13609999418258667\n",
      "run: 177\n",
      "0.12839999794960022\n",
      "run: 178\n",
      "0.13609999418258667\n",
      "run: 179\n",
      "0.13609999418258667\n",
      "run: 180\n",
      "0.1136000007390976\n",
      "run: 181\n",
      "0.11949999630451202\n",
      "run: 182\n",
      "0.11949999630451202\n",
      "run: 183\n",
      "0.12839999794960022\n",
      "run: 184\n",
      "0.13609999418258667\n",
      "run: 185\n",
      "0.13609999418258667\n",
      "run: 186\n",
      "0.12839999794960022\n",
      "run: 187\n",
      "0.13609999418258667\n",
      "run: 188\n",
      "0.13609999418258667\n",
      "run: 189\n",
      "0.10170000046491623\n",
      "run: 190\n",
      "0.1160999983549118\n",
      "run: 191\n",
      "0.1160999983549118\n",
      "run: 192\n",
      "0.11490000039339066\n",
      "run: 193\n",
      "0.1429000049829483\n",
      "run: 194\n",
      "0.1429000049829483\n",
      "run: 195\n",
      "0.11490000039339066\n",
      "run: 196\n",
      "0.1429000049829483\n",
      "run: 197\n",
      "0.1429000049829483\n",
      "run: 198\n",
      "0.12349999696016312\n",
      "run: 199\n",
      "0.12559999525547028\n",
      "run: 200\n",
      "0.12559999525547028\n",
      "run: 201\n",
      "0.14900000393390656\n",
      "run: 202\n",
      "0.14869999885559082\n",
      "run: 203\n",
      "0.14869999885559082\n",
      "run: 204\n",
      "0.14900000393390656\n",
      "run: 205\n",
      "0.14869999885559082\n",
      "run: 206\n",
      "0.14869999885559082\n",
      "run: 207\n",
      "0.12349999696016312\n",
      "run: 208\n",
      "0.12559999525547028\n",
      "run: 209\n",
      "0.12559999525547028\n",
      "run: 210\n",
      "0.14900000393390656\n",
      "run: 211\n",
      "0.14869999885559082\n",
      "run: 212\n",
      "0.14869999885559082\n",
      "run: 213\n",
      "0.14900000393390656\n",
      "run: 214\n",
      "0.14869999885559082\n",
      "run: 215\n",
      "0.14869999885559082\n",
      "run: 216\n",
      "0.10170000046491623\n",
      "run: 217\n",
      "0.1160999983549118\n",
      "run: 218\n",
      "0.1160999983549118\n",
      "run: 219\n",
      "0.11490000039339066\n",
      "run: 220\n",
      "0.1429000049829483\n",
      "run: 221\n",
      "0.1429000049829483\n",
      "run: 222\n",
      "0.11490000039339066\n",
      "run: 223\n",
      "0.1429000049829483\n",
      "run: 224\n",
      "0.1429000049829483\n",
      "run: 225\n",
      "0.12349999696016312\n",
      "run: 226\n",
      "0.12559999525547028\n",
      "run: 227\n",
      "0.12559999525547028\n",
      "run: 228\n",
      "0.14900000393390656\n",
      "run: 229\n",
      "0.14869999885559082\n",
      "run: 230\n",
      "0.14869999885559082\n",
      "run: 231\n",
      "0.14900000393390656\n",
      "run: 232\n",
      "0.14869999885559082\n",
      "run: 233\n",
      "0.14869999885559082\n",
      "run: 234\n",
      "0.12349999696016312\n",
      "run: 235\n",
      "0.12559999525547028\n",
      "run: 236\n",
      "0.12559999525547028\n",
      "run: 237\n",
      "0.14900000393390656\n",
      "run: 238\n",
      "0.14869999885559082\n",
      "run: 239\n",
      "0.14869999885559082\n",
      "run: 240\n",
      "0.14900000393390656\n",
      "run: 241\n",
      "0.14869999885559082\n",
      "run: 242\n",
      "0.14869999885559082\n",
      "run: 243\n",
      "0.10000000149011612\n",
      "run: 244\n",
      "0.1005999967455864\n",
      "run: 245\n",
      "0.1005999967455864\n",
      "run: 246\n",
      "0.10010000318288803\n",
      "run: 247\n",
      "0.13089999556541443\n",
      "run: 248\n",
      "0.13089999556541443\n",
      "run: 249\n",
      "0.10010000318288803\n",
      "run: 250\n",
      "0.13089999556541443\n",
      "run: 251\n",
      "0.13089999556541443\n",
      "run: 252\n",
      "0.10040000081062317\n",
      "run: 253\n",
      "0.1071000024676323\n",
      "run: 254\n",
      "0.1071000024676323\n",
      "run: 255\n",
      "0.10429999977350235\n",
      "run: 256\n",
      "0.1574999988079071\n",
      "run: 257\n",
      "0.1574999988079071\n",
      "run: 258\n",
      "0.10429999977350235\n",
      "run: 259\n",
      "0.1574999988079071\n",
      "run: 260\n",
      "0.1574999988079071\n",
      "run: 261\n",
      "0.10040000081062317\n",
      "run: 262\n",
      "0.1071000024676323\n",
      "run: 263\n",
      "0.1071000024676323\n",
      "run: 264\n",
      "0.10429999977350235\n",
      "run: 265\n",
      "0.1574999988079071\n",
      "run: 266\n",
      "0.1574999988079071\n",
      "run: 267\n",
      "0.10429999977350235\n",
      "run: 268\n",
      "0.1574999988079071\n",
      "run: 269\n",
      "0.1574999988079071\n",
      "run: 270\n",
      "0.10109999775886536\n",
      "run: 271\n",
      "0.11990000307559967\n",
      "run: 272\n",
      "0.11990000307559967\n",
      "run: 273\n",
      "0.1151999980211258\n",
      "run: 274\n",
      "0.17710000276565552\n",
      "run: 275\n",
      "0.17710000276565552\n",
      "run: 276\n",
      "0.1151999980211258\n",
      "run: 277\n",
      "0.17710000276565552\n",
      "run: 278\n",
      "0.17710000276565552\n",
      "run: 279\n",
      "0.11249999701976776\n",
      "run: 280\n",
      "0.1412000060081482\n",
      "run: 281\n",
      "0.1412000060081482\n",
      "run: 282\n",
      "0.15600000321865082\n",
      "run: 283\n",
      "0.17980000376701355\n",
      "run: 284\n",
      "0.17980000376701355\n",
      "run: 285\n",
      "0.15600000321865082\n",
      "run: 286\n",
      "0.17980000376701355\n",
      "run: 287\n",
      "0.17980000376701355\n",
      "run: 288\n",
      "0.11249999701976776\n",
      "run: 289\n",
      "0.1412000060081482\n",
      "run: 290\n",
      "0.1412000060081482\n",
      "run: 291\n",
      "0.15600000321865082\n",
      "run: 292\n",
      "0.17980000376701355\n",
      "run: 293\n",
      "0.17980000376701355\n",
      "run: 294\n",
      "0.15600000321865082\n",
      "run: 295\n",
      "0.17980000376701355\n",
      "run: 296\n",
      "0.17980000376701355\n",
      "run: 297\n",
      "0.10109999775886536\n",
      "run: 298\n",
      "0.11990000307559967\n",
      "run: 299\n",
      "0.11990000307559967\n",
      "run: 300\n",
      "0.1151999980211258\n",
      "run: 301\n",
      "0.17710000276565552\n",
      "run: 302\n",
      "0.17710000276565552\n",
      "run: 303\n",
      "0.1151999980211258\n",
      "run: 304\n",
      "0.17710000276565552\n",
      "run: 305\n",
      "0.17710000276565552\n",
      "run: 306\n",
      "0.11249999701976776\n",
      "run: 307\n",
      "0.1412000060081482\n",
      "run: 308\n",
      "0.1412000060081482\n",
      "run: 309\n",
      "0.15600000321865082\n",
      "run: 310\n",
      "0.17980000376701355\n",
      "run: 311\n",
      "0.17980000376701355\n",
      "run: 312\n",
      "0.15600000321865082\n",
      "run: 313\n",
      "0.17980000376701355\n",
      "run: 314\n",
      "0.17980000376701355\n",
      "run: 315\n",
      "0.11249999701976776\n",
      "run: 316\n",
      "0.1412000060081482\n",
      "run: 317\n",
      "0.1412000060081482\n",
      "run: 318\n",
      "0.15600000321865082\n",
      "run: 319\n",
      "0.17980000376701355\n",
      "run: 320\n",
      "0.17980000376701355\n",
      "run: 321\n",
      "0.15600000321865082\n",
      "run: 322\n",
      "0.17980000376701355\n",
      "run: 323\n",
      "0.17980000376701355\n",
      "run: 324\n",
      "0.10450000315904617\n",
      "run: 325\n",
      "0.14800000190734863\n",
      "run: 326\n",
      "0.14800000190734863\n",
      "run: 327\n",
      "0.1688999980688095\n",
      "run: 328\n",
      "0.29420000314712524\n",
      "run: 329\n",
      "0.29420000314712524\n",
      "run: 330\n",
      "0.1688999980688095\n",
      "run: 331\n",
      "0.29420000314712524\n",
      "run: 332\n",
      "0.29420000314712524\n",
      "run: 333\n",
      "0.14470000565052032\n",
      "run: 334\n",
      "0.257099986076355\n",
      "run: 335\n",
      "0.257099986076355\n",
      "run: 336\n",
      "0.32019999623298645\n",
      "run: 337\n",
      "0.4562999904155731\n",
      "run: 338\n",
      "0.4562999904155731\n",
      "run: 339\n",
      "0.32019999623298645\n",
      "run: 340\n",
      "0.4562999904155731\n",
      "run: 341\n",
      "0.4562999904155731\n",
      "run: 342\n",
      "0.14470000565052032\n",
      "run: 343\n",
      "0.257099986076355\n",
      "run: 344\n",
      "0.257099986076355\n",
      "run: 345\n",
      "0.32019999623298645\n",
      "run: 346\n",
      "0.4562999904155731\n",
      "run: 347\n",
      "0.4562999904155731\n",
      "run: 348\n",
      "0.32019999623298645\n",
      "run: 349\n",
      "0.4562999904155731\n",
      "run: 350\n",
      "0.4562999904155731\n",
      "run: 351\n",
      "0.14949999749660492\n",
      "run: 352\n",
      "0.26759999990463257\n",
      "run: 353\n",
      "0.26759999990463257\n",
      "run: 354\n",
      "0.3264000117778778\n",
      "run: 355\n",
      "0.49320000410079956\n",
      "run: 356\n",
      "0.49320000410079956\n",
      "run: 357\n",
      "0.3264000117778778\n",
      "run: 358\n",
      "0.49320000410079956\n",
      "run: 359\n",
      "0.49320000410079956\n",
      "run: 360\n",
      "0.2572999894618988\n",
      "run: 361\n",
      "0.42170000076293945\n",
      "run: 362\n",
      "0.42170000076293945\n",
      "run: 363\n",
      "0.4643000066280365\n",
      "run: 364\n",
      "0.5961999893188477\n",
      "run: 365\n",
      "0.5961999893188477\n",
      "run: 366\n",
      "0.4643000066280365\n",
      "run: 367\n",
      "0.5961999893188477\n",
      "run: 368\n",
      "0.5961999893188477\n",
      "run: 369\n",
      "0.2572999894618988\n",
      "run: 370\n",
      "0.42170000076293945\n",
      "run: 371\n",
      "0.42170000076293945\n",
      "run: 372\n",
      "0.4643000066280365\n",
      "run: 373\n",
      "0.5961999893188477\n",
      "run: 374\n",
      "0.5961999893188477\n",
      "run: 375\n",
      "0.4643000066280365\n",
      "run: 376\n",
      "0.5961999893188477\n",
      "run: 377\n",
      "0.5961999893188477\n",
      "run: 378\n",
      "0.14949999749660492\n",
      "run: 379\n",
      "0.26759999990463257\n",
      "run: 380\n",
      "0.26759999990463257\n",
      "run: 381\n",
      "0.3264000117778778\n",
      "run: 382\n",
      "0.49320000410079956\n",
      "run: 383\n",
      "0.49320000410079956\n",
      "run: 384\n",
      "0.3264000117778778\n",
      "run: 385\n",
      "0.49320000410079956\n",
      "run: 386\n",
      "0.49320000410079956\n",
      "run: 387\n",
      "0.2572999894618988\n",
      "run: 388\n",
      "0.42170000076293945\n",
      "run: 389\n",
      "0.42170000076293945\n",
      "run: 390\n",
      "0.4643000066280365\n",
      "run: 391\n",
      "0.5961999893188477\n",
      "run: 392\n",
      "0.5961999893188477\n",
      "run: 393\n",
      "0.4643000066280365\n",
      "run: 394\n",
      "0.5961999893188477\n",
      "run: 395\n",
      "0.5961999893188477\n",
      "run: 396\n",
      "0.2572999894618988\n",
      "run: 397\n",
      "0.42170000076293945\n",
      "run: 398\n",
      "0.42170000076293945\n",
      "run: 399\n",
      "0.4643000066280365\n",
      "run: 400\n",
      "0.5961999893188477\n",
      "run: 401\n",
      "0.5961999893188477\n",
      "run: 402\n",
      "0.4643000066280365\n",
      "run: 403\n",
      "0.5961999893188477\n",
      "run: 404\n",
      "0.5961999893188477\n",
      "run: 405\n",
      "0.10450000315904617\n",
      "run: 406\n",
      "0.14800000190734863\n",
      "run: 407\n",
      "0.14800000190734863\n",
      "run: 408\n",
      "0.1688999980688095\n",
      "run: 409\n",
      "0.29420000314712524\n",
      "run: 410\n",
      "0.29420000314712524\n",
      "run: 411\n",
      "0.1688999980688095\n",
      "run: 412\n",
      "0.29420000314712524\n",
      "run: 413\n",
      "0.29420000314712524\n",
      "run: 414\n",
      "0.14470000565052032\n",
      "run: 415\n",
      "0.257099986076355\n",
      "run: 416\n",
      "0.257099986076355\n",
      "run: 417\n",
      "0.32019999623298645\n",
      "run: 418\n",
      "0.4562999904155731\n",
      "run: 419\n",
      "0.4562999904155731\n",
      "run: 420\n",
      "0.32019999623298645\n",
      "run: 421\n",
      "0.4562999904155731\n",
      "run: 422\n",
      "0.4562999904155731\n",
      "run: 423\n",
      "0.14470000565052032\n",
      "run: 424\n",
      "0.257099986076355\n",
      "run: 425\n",
      "0.257099986076355\n",
      "run: 426\n",
      "0.32019999623298645\n",
      "run: 427\n",
      "0.4562999904155731\n",
      "run: 428\n",
      "0.4562999904155731\n",
      "run: 429\n",
      "0.32019999623298645\n",
      "run: 430\n",
      "0.4562999904155731\n",
      "run: 431\n",
      "0.4562999904155731\n",
      "run: 432\n",
      "0.14949999749660492\n",
      "run: 433\n",
      "0.26759999990463257\n",
      "run: 434\n",
      "0.26759999990463257\n",
      "run: 435\n",
      "0.3264000117778778\n",
      "run: 436\n",
      "0.49320000410079956\n",
      "run: 437\n",
      "0.49320000410079956\n",
      "run: 438\n",
      "0.3264000117778778\n",
      "run: 439\n",
      "0.49320000410079956\n",
      "run: 440\n",
      "0.49320000410079956\n",
      "run: 441\n",
      "0.2572999894618988\n",
      "run: 442\n",
      "0.42170000076293945\n",
      "run: 443\n",
      "0.42170000076293945\n",
      "run: 444\n",
      "0.4643000066280365\n",
      "run: 445\n",
      "0.5961999893188477\n",
      "run: 446\n",
      "0.5961999893188477\n",
      "run: 447\n",
      "0.4643000066280365\n",
      "run: 448\n",
      "0.5961999893188477\n",
      "run: 449\n",
      "0.5961999893188477\n",
      "run: 450\n",
      "0.2572999894618988\n",
      "run: 451\n",
      "0.42170000076293945\n",
      "run: 452\n",
      "0.42170000076293945\n",
      "run: 453\n",
      "0.4643000066280365\n",
      "run: 454\n",
      "0.5961999893188477\n",
      "run: 455\n",
      "0.5961999893188477\n",
      "run: 456\n",
      "0.4643000066280365\n",
      "run: 457\n",
      "0.5961999893188477\n",
      "run: 458\n",
      "0.5961999893188477\n",
      "run: 459\n",
      "0.14949999749660492\n",
      "run: 460\n",
      "0.26759999990463257\n",
      "run: 461\n",
      "0.26759999990463257\n",
      "run: 462\n",
      "0.3264000117778778\n",
      "run: 463\n",
      "0.49320000410079956\n",
      "run: 464\n",
      "0.49320000410079956\n",
      "run: 465\n",
      "0.3264000117778778\n",
      "run: 466\n",
      "0.49320000410079956\n",
      "run: 467\n",
      "0.49320000410079956\n",
      "run: 468\n",
      "0.2572999894618988\n",
      "run: 469\n",
      "0.42170000076293945\n",
      "run: 470\n",
      "0.42170000076293945\n",
      "run: 471\n",
      "0.4643000066280365\n",
      "run: 472\n",
      "0.5961999893188477\n",
      "run: 473\n",
      "0.5961999893188477\n",
      "run: 474\n",
      "0.4643000066280365\n",
      "run: 475\n",
      "0.5961999893188477\n",
      "run: 476\n",
      "0.5961999893188477\n",
      "run: 477\n",
      "0.2572999894618988\n",
      "run: 478\n",
      "0.42170000076293945\n",
      "run: 479\n",
      "0.42170000076293945\n",
      "run: 480\n",
      "0.4643000066280365\n",
      "run: 481\n",
      "0.5961999893188477\n",
      "run: 482\n",
      "0.5961999893188477\n",
      "run: 483\n",
      "0.4643000066280365\n",
      "run: 484\n",
      "0.5961999893188477\n",
      "run: 485\n",
      "0.5961999893188477\n",
      "run: 486\n",
      "0.10000000149011612\n",
      "run: 487\n",
      "0.10350000113248825\n",
      "run: 488\n",
      "0.10350000113248825\n",
      "run: 489\n",
      "0.10170000046491623\n",
      "run: 490\n",
      "0.14229999482631683\n",
      "run: 491\n",
      "0.14229999482631683\n",
      "run: 492\n",
      "0.10170000046491623\n",
      "run: 493\n",
      "0.14229999482631683\n",
      "run: 494\n",
      "0.14229999482631683\n",
      "run: 495\n",
      "0.10409999638795853\n",
      "run: 496\n",
      "0.11670000106096268\n",
      "run: 497\n",
      "0.11670000106096268\n",
      "run: 498\n",
      "0.125900000333786\n",
      "run: 499\n",
      "0.19850000739097595\n",
      "run: 500\n",
      "0.19850000739097595\n",
      "run: 501\n",
      "0.125900000333786\n",
      "run: 502\n",
      "0.19850000739097595\n",
      "run: 503\n",
      "0.19850000739097595\n",
      "run: 504\n",
      "0.10409999638795853\n",
      "run: 505\n",
      "0.11670000106096268\n",
      "run: 506\n",
      "0.11670000106096268\n",
      "run: 507\n",
      "0.125900000333786\n",
      "run: 508\n",
      "0.19850000739097595\n",
      "run: 509\n",
      "0.19850000739097595\n",
      "run: 510\n",
      "0.125900000333786\n",
      "run: 511\n",
      "0.19850000739097595\n",
      "run: 512\n",
      "0.19850000739097595\n",
      "run: 513\n",
      "0.10440000146627426\n",
      "run: 514\n",
      "0.14409999549388885\n",
      "run: 515\n",
      "0.14409999549388885\n",
      "run: 516\n",
      "0.15320000052452087\n",
      "run: 517\n",
      "0.26159998774528503\n",
      "run: 518\n",
      "0.26159998774528503\n",
      "run: 519\n",
      "0.15320000052452087\n",
      "run: 520\n",
      "0.26159998774528503\n",
      "run: 521\n",
      "0.26159998774528503\n",
      "run: 522\n",
      "0.13760000467300415\n",
      "run: 523\n",
      "0.19210000336170197\n",
      "run: 524\n",
      "0.19210000336170197\n",
      "run: 525\n",
      "0.2442999929189682\n",
      "run: 526\n",
      "0.30559998750686646\n",
      "run: 527\n",
      "0.30559998750686646\n",
      "run: 528\n",
      "0.2442999929189682\n",
      "run: 529\n",
      "0.30559998750686646\n",
      "run: 530\n",
      "0.30559998750686646\n",
      "run: 531\n",
      "0.13760000467300415\n",
      "run: 532\n",
      "0.19210000336170197\n",
      "run: 533\n",
      "0.19210000336170197\n",
      "run: 534\n",
      "0.2442999929189682\n",
      "run: 535\n",
      "0.30559998750686646\n",
      "run: 536\n",
      "0.30559998750686646\n",
      "run: 537\n",
      "0.2442999929189682\n",
      "run: 538\n",
      "0.30559998750686646\n",
      "run: 539\n",
      "0.30559998750686646\n",
      "run: 540\n",
      "0.10440000146627426\n",
      "run: 541\n",
      "0.14409999549388885\n",
      "run: 542\n",
      "0.14409999549388885\n",
      "run: 543\n",
      "0.15320000052452087\n",
      "run: 544\n",
      "0.26159998774528503\n",
      "run: 545\n",
      "0.26159998774528503\n",
      "run: 546\n",
      "0.15320000052452087\n",
      "run: 547\n",
      "0.26159998774528503\n",
      "run: 548\n",
      "0.26159998774528503\n",
      "run: 549\n",
      "0.13760000467300415\n",
      "run: 550\n",
      "0.19210000336170197\n",
      "run: 551\n",
      "0.19210000336170197\n",
      "run: 552\n",
      "0.2442999929189682\n",
      "run: 553\n",
      "0.30559998750686646\n",
      "run: 554\n",
      "0.30559998750686646\n",
      "run: 555\n",
      "0.2442999929189682\n",
      "run: 556\n",
      "0.30559998750686646\n",
      "run: 557\n",
      "0.30559998750686646\n",
      "run: 558\n",
      "0.13760000467300415\n",
      "run: 559\n",
      "0.19210000336170197\n",
      "run: 560\n",
      "0.19210000336170197\n",
      "run: 561\n",
      "0.2442999929189682\n",
      "run: 562\n",
      "0.30559998750686646\n",
      "run: 563\n",
      "0.30559998750686646\n",
      "run: 564\n",
      "0.2442999929189682\n",
      "run: 565\n",
      "0.30559998750686646\n",
      "run: 566\n",
      "0.30559998750686646\n",
      "run: 567\n",
      "0.11069999635219574\n",
      "run: 568\n",
      "0.1987999975681305\n",
      "run: 569\n",
      "0.1987999975681305\n",
      "run: 570\n",
      "0.23100000619888306\n",
      "run: 571\n",
      "0.40220001339912415\n",
      "run: 572\n",
      "0.40220001339912415\n",
      "run: 573\n",
      "0.23100000619888306\n",
      "run: 574\n",
      "0.40220001339912415\n",
      "run: 575\n",
      "0.40220001339912415\n",
      "run: 576\n",
      "0.18140000104904175\n",
      "run: 577\n",
      "0.3564000129699707\n",
      "run: 578\n",
      "0.3564000129699707\n",
      "run: 579\n",
      "0.4163999855518341\n",
      "run: 580\n",
      "0.5738999843597412\n",
      "run: 581\n",
      "0.5738999843597412\n",
      "run: 582\n",
      "0.4163999855518341\n",
      "run: 583\n",
      "0.5738999843597412\n",
      "run: 584\n",
      "0.5738999843597412\n",
      "run: 585\n",
      "0.18140000104904175\n",
      "run: 586\n",
      "0.3564000129699707\n",
      "run: 587\n",
      "0.3564000129699707\n",
      "run: 588\n",
      "0.4163999855518341\n",
      "run: 589\n",
      "0.5738999843597412\n",
      "run: 590\n",
      "0.5738999843597412\n",
      "run: 591\n",
      "0.4163999855518341\n",
      "run: 592\n",
      "0.5738999843597412\n",
      "run: 593\n",
      "0.5738999843597412\n",
      "run: 594\n",
      "0.19269999861717224\n",
      "run: 595\n",
      "0.3691999912261963\n",
      "run: 596\n",
      "0.3691999912261963\n",
      "run: 597\n",
      "0.4320000112056732\n",
      "run: 598\n",
      "0.6093999743461609\n",
      "run: 599\n",
      "0.6093999743461609\n",
      "run: 600\n",
      "0.4320000112056732\n",
      "run: 601\n",
      "0.6093999743461609\n",
      "run: 602\n",
      "0.6093999743461609\n",
      "run: 603\n",
      "0.3314000070095062\n",
      "run: 604\n",
      "0.539900004863739\n",
      "run: 605\n",
      "0.539900004863739\n",
      "run: 606\n",
      "0.5583000183105469\n",
      "run: 607\n",
      "0.7106000185012817\n",
      "run: 608\n",
      "0.7106000185012817\n",
      "run: 609\n",
      "0.5583000183105469\n",
      "run: 610\n",
      "0.7106000185012817\n",
      "run: 611\n",
      "0.7106000185012817\n",
      "run: 612\n",
      "0.3314000070095062\n",
      "run: 613\n",
      "0.539900004863739\n",
      "run: 614\n",
      "0.539900004863739\n",
      "run: 615\n",
      "0.5583000183105469\n",
      "run: 616\n",
      "0.7106000185012817\n",
      "run: 617\n",
      "0.7106000185012817\n",
      "run: 618\n",
      "0.5583000183105469\n",
      "run: 619\n",
      "0.7106000185012817\n",
      "run: 620\n",
      "0.7106000185012817\n",
      "run: 621\n",
      "0.19269999861717224\n",
      "run: 622\n",
      "0.3691999912261963\n",
      "run: 623\n",
      "0.3691999912261963\n",
      "run: 624\n",
      "0.4320000112056732\n",
      "run: 625\n",
      "0.6093999743461609\n",
      "run: 626\n",
      "0.6093999743461609\n",
      "run: 627\n",
      "0.4320000112056732\n",
      "run: 628\n",
      "0.6093999743461609\n",
      "run: 629\n",
      "0.6093999743461609\n",
      "run: 630\n",
      "0.3314000070095062\n",
      "run: 631\n",
      "0.539900004863739\n",
      "run: 632\n",
      "0.539900004863739\n",
      "run: 633\n",
      "0.5583000183105469\n",
      "run: 634\n",
      "0.7106000185012817\n",
      "run: 635\n",
      "0.7106000185012817\n",
      "run: 636\n",
      "0.5583000183105469\n",
      "run: 637\n",
      "0.7106000185012817\n",
      "run: 638\n",
      "0.7106000185012817\n",
      "run: 639\n",
      "0.3314000070095062\n",
      "run: 640\n",
      "0.539900004863739\n",
      "run: 641\n",
      "0.539900004863739\n",
      "run: 642\n",
      "0.5583000183105469\n",
      "run: 643\n",
      "0.7106000185012817\n",
      "run: 644\n",
      "0.7106000185012817\n",
      "run: 645\n",
      "0.5583000183105469\n",
      "run: 646\n",
      "0.7106000185012817\n",
      "run: 647\n",
      "0.7106000185012817\n",
      "run: 648\n",
      "0.11069999635219574\n",
      "run: 649\n",
      "0.1987999975681305\n",
      "run: 650\n",
      "0.1987999975681305\n",
      "run: 651\n",
      "0.23100000619888306\n",
      "run: 652\n",
      "0.40220001339912415\n",
      "run: 653\n",
      "0.40220001339912415\n",
      "run: 654\n",
      "0.23100000619888306\n",
      "run: 655\n",
      "0.40220001339912415\n",
      "run: 656\n",
      "0.40220001339912415\n",
      "run: 657\n",
      "0.18140000104904175\n",
      "run: 658\n",
      "0.3564000129699707\n",
      "run: 659\n",
      "0.3564000129699707\n",
      "run: 660\n",
      "0.4163999855518341\n",
      "run: 661\n",
      "0.5738999843597412\n",
      "run: 662\n",
      "0.5738999843597412\n",
      "run: 663\n",
      "0.4163999855518341\n",
      "run: 664\n",
      "0.5738999843597412\n",
      "run: 665\n",
      "0.5738999843597412\n",
      "run: 666\n",
      "0.18140000104904175\n",
      "run: 667\n",
      "0.3564000129699707\n",
      "run: 668\n",
      "0.3564000129699707\n",
      "run: 669\n",
      "0.4163999855518341\n",
      "run: 670\n",
      "0.5738999843597412\n",
      "run: 671\n",
      "0.5738999843597412\n",
      "run: 672\n",
      "0.4163999855518341\n",
      "run: 673\n",
      "0.5738999843597412\n",
      "run: 674\n",
      "0.5738999843597412\n",
      "run: 675\n",
      "0.19269999861717224\n",
      "run: 676\n",
      "0.3691999912261963\n",
      "run: 677\n",
      "0.3691999912261963\n",
      "run: 678\n",
      "0.4320000112056732\n",
      "run: 679\n",
      "0.6093999743461609\n",
      "run: 680\n",
      "0.6093999743461609\n",
      "run: 681\n",
      "0.4320000112056732\n",
      "run: 682\n",
      "0.6093999743461609\n",
      "run: 683\n",
      "0.6093999743461609\n",
      "run: 684\n",
      "0.3314000070095062\n",
      "run: 685\n",
      "0.539900004863739\n",
      "run: 686\n",
      "0.539900004863739\n",
      "run: 687\n",
      "0.5583000183105469\n",
      "run: 688\n",
      "0.7106000185012817\n",
      "run: 689\n",
      "0.7106000185012817\n",
      "run: 690\n",
      "0.5583000183105469\n",
      "run: 691\n",
      "0.7106000185012817\n",
      "run: 692\n",
      "0.7106000185012817\n",
      "run: 693\n",
      "0.3314000070095062\n",
      "run: 694\n",
      "0.539900004863739\n",
      "run: 695\n",
      "0.539900004863739\n",
      "run: 696\n",
      "0.5583000183105469\n",
      "run: 697\n",
      "0.7106000185012817\n",
      "run: 698\n",
      "0.7106000185012817\n",
      "run: 699\n",
      "0.5583000183105469\n",
      "run: 700\n",
      "0.7106000185012817\n",
      "run: 701\n",
      "0.7106000185012817\n",
      "run: 702\n",
      "0.19269999861717224\n",
      "run: 703\n",
      "0.3691999912261963\n",
      "run: 704\n",
      "0.3691999912261963\n",
      "run: 705\n",
      "0.4320000112056732\n",
      "run: 706\n",
      "0.6093999743461609\n",
      "run: 707\n",
      "0.6093999743461609\n",
      "run: 708\n",
      "0.4320000112056732\n",
      "run: 709\n",
      "0.6093999743461609\n",
      "run: 710\n",
      "0.6093999743461609\n",
      "run: 711\n",
      "0.3314000070095062\n",
      "run: 712\n",
      "0.539900004863739\n",
      "run: 713\n",
      "0.539900004863739\n",
      "run: 714\n",
      "0.5583000183105469\n",
      "run: 715\n",
      "0.7106000185012817\n",
      "run: 716\n",
      "0.7106000185012817\n",
      "run: 717\n",
      "0.5583000183105469\n",
      "run: 718\n",
      "0.7106000185012817\n",
      "run: 719\n",
      "0.7106000185012817\n",
      "run: 720\n",
      "0.3314000070095062\n",
      "run: 721\n",
      "0.539900004863739\n",
      "run: 722\n",
      "0.539900004863739\n",
      "run: 723\n",
      "0.5583000183105469\n",
      "run: 724\n",
      "0.7106000185012817\n",
      "run: 725\n",
      "0.7106000185012817\n",
      "run: 726\n",
      "0.5583000183105469\n",
      "run: 727\n",
      "0.7106000185012817\n",
      "run: 728\n",
      "0.7106000185012817\n",
      "729 729\n",
      "new accuracy\n",
      "0.7106000185012817\n",
      "mean used Bits\n",
      "8.0\n"
     ]
    }
   ],
   "source": [
    "###second Version (testing every permutation)\n",
    "\n",
    "#set up needed variables\n",
    "modelCpy = copy.deepcopy(model)\n",
    "layerCount = len(model.layers)\n",
    "fractBitsSearchSpace = [2,4,8]\n",
    "tolerance = 0.1\n",
    "decimalBits = []\n",
    "nonEmptyLayerIdx = []\n",
    "accuracies = []\n",
    "\n",
    "#get indices of all layers with weights\n",
    "for idx in range(layerCount):\n",
    "    if (modelCpy.layers[idx].get_weights() != []):\n",
    "        nonEmptyLayerIdx.append(idx)\n",
    "\n",
    "print(\"non-empty-Layers:\")\n",
    "print(nonEmptyLayerIdx)\n",
    "\n",
    "#determine the needed decimal bits\n",
    "for layerIdx in range(layerCount):\n",
    "    #case for empty layers\n",
    "    if not(layerIdx in nonEmptyLayerIdx):\n",
    "        decimalBits.append(0)\n",
    "    else:\n",
    "        #determine log2 of maximum range\n",
    "        minimum = math.floor(min(np.min(modelCpy.layers[layerIdx].get_weights()[0]), np.min(modelCpy.layers[layerIdx].get_weights()[1])))  \n",
    "        maximum = math.ceil(max(np.max(modelCpy.layers[layerIdx].get_weights()[0]), np.max(modelCpy.layers[layerIdx].get_weights()[1])))\n",
    "        valueRange = max(maximum, 0) - min(minimum, 0)\n",
    "        #safety check\n",
    "        if (valueRange > 0):\n",
    "            decimalBits.append(math.ceil(math.log(valueRange, 2)))\n",
    "        else:\n",
    "            print(\"Error: Value Range is 0 or negative\")\n",
    "\n",
    "print(\"decimal Bits:\")\n",
    "print(decimalBits)\n",
    "\n",
    "#iterate over all permutations and test the accuracy\n",
    "permutations = list(iter.product(fractBitsSearchSpace, repeat = len(nonEmptyLayerIdx)))\n",
    "print(\"goal: \" + str(len(permutations)) + \" permutations\")\n",
    "for permutation in range(len(permutations)):\n",
    "    for layerIdx in nonEmptyLayerIdx:\n",
    "        currentFractBits = permutations[permutation][nonEmptyLayerIdx.index(layerIdx)]\n",
    "        newLayer = []\n",
    "        newLayer.append(fixedPointConversion(copy.deepcopy(model.layers[layerIdx].get_weights()[0]), decimalBits[layerIdx] + currentFractBits, currentFractBits))\n",
    "        newLayer.append(fixedPointConversion(copy.deepcopy(model.layers[layerIdx].get_weights()[1]), decimalBits[layerIdx] + currentFractBits, currentFractBits))\n",
    "        modelCpy.layers[layerIdx].set_weights(newLayer)\n",
    "    test_loss, test_acc = modelCpy.evaluate(testImages,  testLabels, verbose=0)\n",
    "    accuracies.append(test_acc)\n",
    "    print(\"run: \" + str(permutation))\n",
    "    print(test_acc)\n",
    "\n",
    "print (\"length of accuracies and permutations. should be equal:\")\n",
    "print(len(accuracies), len(permutations))\n",
    "\n",
    "#determine best suited Values by computing the ratios of bits to accuracy\n",
    "ratios = []\n",
    "minimumAccuracy = max(accuracies) - tolerance\n",
    "for idx in range(len(permutations)):\n",
    "    if (accuracies[idx] < minimumAccuracy):\n",
    "        ratios.append(0)\n",
    "    else:\n",
    "        ratios.append(sum(permutations[idx]) / accuracies[idx])\n",
    "\n",
    "bestPermutation = permutations[ratios.index(max(ratios))]\n",
    "for layerIdx in nonEmptyLayerIdx:\n",
    "        currentFractBits = bestPermutation[nonEmptyLayerIdx.index(layerIdx)]\n",
    "        newLayer = []\n",
    "        newLayer.append(fixedPointConversion(copy.deepcopy(model.layers[layerIdx].get_weights()[0]), decimalBits[layerIdx] + currentFractBits, currentFractBits))\n",
    "        newLayer.append(fixedPointConversion(copy.deepcopy(model.layers[layerIdx].get_weights()[1]), decimalBits[layerIdx] + currentFractBits, currentFractBits))\n",
    "        modelCpy.layers[layerIdx].set_weights(newLayer)\n",
    "\n",
    "test_loss, test_acc = modelCpy.evaluate(testImages,  testLabels, verbose=0)\n",
    "print(\"new accuracy\")\n",
    "print(test_acc)\n",
    "print(\"mean used Bits\")\n",
    "print(sum(bestPermutation)/len(bestPermutation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 25 variables whereas the saved optimizer has 1 variables. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-empty-Layers:\n",
      "[0, 3, 6, 9, 10, 11]\n",
      "decimal Bits:\n",
      "[1, 0, 0, 2, 0, 0, 2, 0, 0, 2, 1, 2]\n",
      "WARNING:tensorflow:From c:\\Users\\7Zylo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\7Zylo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n",
      "WARNING:tensorflow:From c:\\Users\\7Zylo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:\n",
      "0.6779000163078308\n",
      "with mean Bits:\n",
      "17.666666666666668\n"
     ]
    }
   ],
   "source": [
    "###third version (evaluating the needed FractionalBits)\n",
    "\n",
    "#quantization\n",
    "modelCpy = copy.deepcopy(model)\n",
    "layerCount = len(model.layers)\n",
    "fractBitsSearchSpace = [2,4,8]\n",
    "nonEmptyLayerIdx = []\n",
    "decimalBits = []\n",
    "\n",
    "##get indices of all layers with weights\n",
    "for idx in range(layerCount):\n",
    "    if (modelCpy.layers[idx].get_weights() != []):\n",
    "        nonEmptyLayerIdx.append(idx)\n",
    "\n",
    "print(\"non-empty-Layers:\")\n",
    "print(nonEmptyLayerIdx)\n",
    "\n",
    "###todo optimize\n",
    "for layerIdx in range(layerCount):\n",
    "    if not(layerIdx in nonEmptyLayerIdx):\n",
    "        decimalBits.append(0)\n",
    "    else:\n",
    "        minimum = math.floor(min(np.min(modelCpy.layers[layerIdx].get_weights()[0]), np.min(modelCpy.layers[layerIdx].get_weights()[1])))  \n",
    "        maximum = math.ceil(max(np.max(modelCpy.layers[layerIdx].get_weights()[0]), np.max(modelCpy.layers[layerIdx].get_weights()[1])))\n",
    "        valueRange = max(maximum, 0) - min(minimum, 0)\n",
    "        if (valueRange > 0):\n",
    "            decimalBits.append(math.ceil(math.log(valueRange, 2)))\n",
    "        else:\n",
    "            print(\"Error: Value Range is 0 or negative\")\n",
    "\n",
    "print(\"decimal Bits:\")\n",
    "print(decimalBits)\n",
    "\n",
    "chosenBits =[]\n",
    "for layerIdx in nonEmptyLayerIdx:\n",
    "    currentFractBits = computeFractBits(copy.deepcopy(model.layers[layerIdx].get_weights()[0]), copy.deepcopy(model.layers[layerIdx].get_weights()[1]))\n",
    "    chosenBits.append(currentFractBits + decimalBits[layerIdx])\n",
    "    newLayer = []\n",
    "    newLayer.append(fixedPointConversion(copy.deepcopy(model.layers[layerIdx].get_weights()[0]), decimalBits[layerIdx] + currentFractBits, currentFractBits))\n",
    "    newLayer.append(fixedPointConversion(copy.deepcopy(model.layers[layerIdx].get_weights()[1]), decimalBits[layerIdx] + currentFractBits, currentFractBits))\n",
    "    modelCpy.layers[layerIdx].set_weights(newLayer)\n",
    "test_loss, test_acc = modelCpy.evaluate(testImages,  testLabels, verbose=0)\n",
    "print(\"accuracy:\")\n",
    "print(test_acc)\n",
    "print(\"with mean Bits:\")\n",
    "print(np.sum(chosenBits)/len(chosenBits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 0 bits: 6 fractional Bits: 6 Accuracy: \n",
      "0.7366999983787537\n",
      "Layer: 3 bits: 4 fractional Bits: 4 Accuracy: \n",
      "0.7233999967575073\n",
      "Layer: 6 bits: 4 fractional Bits: 4 Accuracy: \n",
      "0.7164000272750854\n",
      "Layer: 9 bits: 4 fractional Bits: 4 Accuracy: \n",
      "0.7146999835968018\n",
      "Layer: 10 bits: 4 fractional Bits: 4 Accuracy: \n",
      "0.7085999846458435\n",
      "Layer: 11 bits: 4 fractional Bits: 4 Accuracy: \n",
      "0.7106000185012817\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average storage use: 4.333333333333333 Bits\n"
     ]
    }
   ],
   "source": [
    "###first Version (layer by layer)\n",
    "fractBitsList = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22]\n",
    "maxBitSize = 24\n",
    "tolerancePerLayer = 0.05\n",
    "layerCount = len(model.layers)\n",
    "averageStorageUse = []\n",
    "accuracyDeveloptment = [test_acc]\n",
    "\n",
    "#optimizes layer by layer\n",
    "for currentLayer in range(layerCount):\n",
    "    originalLayer = copy.deepcopy(model.layers[currentLayer])\n",
    "    #only computes if layer has weights\n",
    "    if (originalLayer.get_weights() != []):\n",
    "        #initializes matrix for storing layer accuracy\n",
    "        layerAccuracy =  np.zeros((maxBitSize, len(fractBitsList)))\n",
    "        #iterates over specified numberfs of fractional bits, and specified number of overall bits for each\n",
    "        for fractBitsIdx in range(len(fractBitsList)):\n",
    "            for bits in range(max(fractBitsList[fractBitsIdx], 1), maxBitSize + 1):\n",
    "                #initializes a new container for all weights of the current layer\n",
    "                newLayer = []\n",
    "                #iterates over all sets of weights in layer (needed, because the list of all weights has inhomogenus shape, so numpy cant handle it)\n",
    "                for weightCount in range(len(originalLayer.get_weights())):\n",
    "                    #append the quantized weights to new layer\n",
    "                    newLayer.append(fixedPointConversion(copy.deepcopy(originalLayer.get_weights()[weightCount]), bits, fractBitsList[fractBitsIdx]))\n",
    "                #sets the new layer\n",
    "                model.layers[currentLayer].set_weights(newLayer)\n",
    "                #tests the new accuracy and stores it in a matrix\n",
    "                test_loss, test_acc = model.evaluate(testImages,  testLabels, verbose=0)\n",
    "                layerAccuracy[bits-1][fractBitsIdx] = test_acc        \n",
    "\n",
    "        #detecting the best accuracy by tolerance\n",
    "        #ToDo: optimize the function picking the best sizes\n",
    "        bestAccuracy = {\"bitIdx\" : 0, \"fractIdx\" : 0, \"accuracy\" : 0}\n",
    "        for bits in range(maxBitSize):\n",
    "            for fractBits in range(len(fractBitsList)):\n",
    "                if ((layerAccuracy[bits][fractBits] - tolerancePerLayer) > bestAccuracy[\"accuracy\"]):\n",
    "                    bestAccuracy[\"bitIdx\"] = bits\n",
    "                    bestAccuracy[\"fractIdx\"] = fractBits\n",
    "                    bestAccuracy[\"accuracy\"] = layerAccuracy[bits][fractBits]\n",
    "        \n",
    "        #set the final new weights\n",
    "        if (bestAccuracy[\"bitIdx\"] > 0):\n",
    "            newLayer = []\n",
    "            for weightCount in range(len(originalLayer.get_weights())):\n",
    "                newLayer.append(fixedPointConversion(copy.deepcopy(originalLayer.get_weights()[weightCount]), bestAccuracy[\"bitIdx\"] + 1, fractBitsList[bestAccuracy[\"fractIdx\"]]))\n",
    "            model.layers[currentLayer].set_weights(newLayer)\n",
    "        print(\"Layer: \" + str(currentLayer) + \" bits: \" + str(bestAccuracy[\"bitIdx\"] + 1) + \" fractional Bits: \" + str(fractBitsList[bestAccuracy[\"fractIdx\"]]) + \" Accuracy: \")\n",
    "        averageStorageUse.append(bestAccuracy[\"bitIdx\"] + 1)\n",
    "        test_loss, test_acc = model.evaluate(testImages,  testLabels, verbose=0)\n",
    "        print (test_acc)\n",
    "        accuracyDeveloptment.append(test_acc)\n",
    "\n",
    "plt.xlabel(\"Quantized Layer\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([0, 1])\n",
    "plt.plot(accuracyDeveloptment)\n",
    "plt.show()\n",
    "\n",
    "print(\"average storage use: \" + str(np.mean(averageStorageUse)) + \" Bits\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
